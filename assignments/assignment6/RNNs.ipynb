{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Week 06 - RNNs, part 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii-fGrc3MSZO"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P59NYU98GCb9"
      },
      "source": [
        "!pip3 -qq install torch\n",
        "!pip3 -qq install bokeh\n",
        "!pip3 -qq install gensim\n",
        "!pip3 -qq install nltk"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sVtGHmA9aBM"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiA2dGmgF1rW",
        "outputId": "71770799-d296-4150-dbc4-bda57f9338ff"
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QstS4NO0L97c",
        "outputId": "b1d214c9-6e7c-483d-e149-f74759e139d6"
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTai8Ta0lgwL",
        "outputId": "d5b96893-d5b3-4d01-ee1a-4bf0004980dc"
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCjwwDs6Zq9x",
        "outputId": "6ecd9a8d-5d25-4ffa-ba8d-363d3779df0b"
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'NOUN', 'ADV', 'PRT', 'ADP', 'X', 'VERB', 'DET', 'PRON', 'CONJ', 'ADJ', 'NUM', '.'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "URC1B2nvPGFt",
        "outputId": "bd836d5d-9b4f-496b-ba29-04a1e4aa513f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rWmSToIaeAo",
        "outputId": "74789191-2c9b-4f62-d501-3cb8c15a3667"
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjz_Rk0bbMyH",
        "outputId": "68f6828f-013a-4fa9-d501-aeb19abb675b"
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XCuxEBVbOY_",
        "outputId": "e0e05938-56f8-4b1b-ba71-4c8f56f20f13"
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtRbz1SwgEqc"
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhsTKZalfih6"
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4XsRII5kW5x",
        "outputId": "c6bde9c1-de9b-4704-93cf-1dc037bc29c7"
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKKyXGDCYkF-"
      },
      "source": [
        "        total_length = padded_input.size(1)  # get the max sequence length\n",
        "        packed_input = pack_padded_sequence(padded_input, input_lengths,\n",
        "                                            batch_first=True)\n",
        "        packed_output, _ = self.my_lstm(packed_input)\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True,\n",
        "                                        total_length=total_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVEHju54d68T"
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(input_size=word_emb_dim,\n",
        "                            hidden_size=lstm_hidden_dim,\n",
        "                            num_layers=lstm_layers_count)\n",
        "        self.fc = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        output = self.embedding(inputs)\n",
        "        output, _ = self.lstm(output)\n",
        "        output = self.fc(output)\n",
        "        return output"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoR4CSh1Eyxr",
        "outputId": "141d7079-92a6-4867-8bb1-e586c72780a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "rnn = nn.LSTM(10, 20, 2) # (input_size, hidden_size, num_layers)\n",
        "input = torch.randn(5, 3, 10) # (seq_len, batch, input_size)\n",
        "h0 = torch.randn(2, 3, 20)\n",
        "c0 = torch.randn(2, 3, 20)\n",
        "output, (hn, cn) = rnn(input, (h0, c0)) \n",
        "# output = (seq_len, batch, num_directions * hidden_size)\n",
        "# hn = (num_layers * num_directions, batch, hidden_size)\n",
        "# cn = (num_layers * num_directions, batch, hidden_size)\n",
        "print(output.shape, hn.shape, cn.shape)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 3, 20]) torch.Size([2, 3, 20]) torch.Size([2, 3, 20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbrxsZ2mehWB",
        "outputId": "f1e74cd1-ee96-4323-f4c0-64befd2af798"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "argmax_logits = logits.view(-1, tagset_size).argmax(dim=1)\n",
        "accuracy = (argmax_logits == y_batch.view(-1)).sum() / y_batch.numel()\n",
        "print(accuracy)"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0234)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMUyUm1hgpe3",
        "outputId": "f49f319b-e923-4604-9c3f-2bff32081c7e"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion(logits.view(-1, 13), y_batch.view(-1))"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.5763, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FprPQ0gllo7b"
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "tagset_size = len(tag2ind)  # n_classes\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "                \n",
        "                y_batch = y_batch.view(-1)\n",
        "                logits = logits.view(-1, tagset_size)\n",
        "\n",
        "                loss = criterion(logits, y_batch)\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                cur_correct_count, cur_sum_count = (logits.argmax(dim=1) == y_batch).sum(), y_batch.numel()\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "                \n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        " \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')\n",
        "        "
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqfbeh1ltEYa",
        "outputId": "ec19fbd2-0de1-428e-b9d1-0da029ef310a"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.68727, Accuracy = 24.53%: 100%|██████████| 572/572 [00:04<00:00, 116.01it/s]\n",
            "[1 / 50]   Val: Loss = 0.35783, Accuracy = 17.81%: 100%|██████████| 13/13 [00:00<00:00, 84.91it/s]\n",
            "[2 / 50] Train: Loss = 0.27401, Accuracy = 28.35%: 100%|██████████| 572/572 [00:04<00:00, 117.13it/s]\n",
            "[2 / 50]   Val: Loss = 0.24532, Accuracy = 18.24%: 100%|██████████| 13/13 [00:00<00:00, 83.44it/s]\n",
            "[3 / 50] Train: Loss = 0.18647, Accuracy = 29.34%: 100%|██████████| 572/572 [00:05<00:00, 113.47it/s]\n",
            "[3 / 50]   Val: Loss = 0.20030, Accuracy = 19.51%: 100%|██████████| 13/13 [00:00<00:00, 82.86it/s]\n",
            "[4 / 50] Train: Loss = 0.13998, Accuracy = 29.86%: 100%|██████████| 572/572 [00:05<00:00, 112.86it/s]\n",
            "[4 / 50]   Val: Loss = 0.17434, Accuracy = 18.59%: 100%|██████████| 13/13 [00:00<00:00, 81.97it/s]\n",
            "[5 / 50] Train: Loss = 0.10932, Accuracy = 30.21%: 100%|██████████| 572/572 [00:04<00:00, 114.52it/s]\n",
            "[5 / 50]   Val: Loss = 0.16818, Accuracy = 17.38%: 100%|██████████| 13/13 [00:00<00:00, 77.90it/s]\n",
            "[6 / 50] Train: Loss = 0.08742, Accuracy = 30.55%: 100%|██████████| 572/572 [00:05<00:00, 112.43it/s]\n",
            "[6 / 50]   Val: Loss = 0.16745, Accuracy = 18.17%: 100%|██████████| 13/13 [00:00<00:00, 80.02it/s]\n",
            "[7 / 50] Train: Loss = 0.07047, Accuracy = 30.70%: 100%|██████████| 572/572 [00:04<00:00, 114.54it/s]\n",
            "[7 / 50]   Val: Loss = 0.16564, Accuracy = 19.17%: 100%|██████████| 13/13 [00:00<00:00, 84.03it/s]\n",
            "[8 / 50] Train: Loss = 0.05712, Accuracy = 31.07%: 100%|██████████| 572/572 [00:05<00:00, 110.99it/s]\n",
            "[8 / 50]   Val: Loss = 0.19058, Accuracy = 18.11%: 100%|██████████| 13/13 [00:00<00:00, 82.37it/s]\n",
            "[9 / 50] Train: Loss = 0.04663, Accuracy = 30.83%: 100%|██████████| 572/572 [00:05<00:00, 110.22it/s]\n",
            "[9 / 50]   Val: Loss = 0.17467, Accuracy = 18.71%: 100%|██████████| 13/13 [00:00<00:00, 83.31it/s]\n",
            "[10 / 50] Train: Loss = 0.03812, Accuracy = 30.95%: 100%|██████████| 572/572 [00:05<00:00, 110.03it/s]\n",
            "[10 / 50]   Val: Loss = 0.17665, Accuracy = 19.22%: 100%|██████████| 13/13 [00:00<00:00, 84.43it/s]\n",
            "[11 / 50] Train: Loss = 0.03133, Accuracy = 31.07%: 100%|██████████| 572/572 [00:05<00:00, 111.41it/s]\n",
            "[11 / 50]   Val: Loss = 0.19383, Accuracy = 17.43%: 100%|██████████| 13/13 [00:00<00:00, 73.50it/s]\n",
            "[12 / 50] Train: Loss = 0.02551, Accuracy = 31.04%: 100%|██████████| 572/572 [00:04<00:00, 114.90it/s]\n",
            "[12 / 50]   Val: Loss = 0.20448, Accuracy = 18.10%: 100%|██████████| 13/13 [00:00<00:00, 77.11it/s]\n",
            "[13 / 50] Train: Loss = 0.02088, Accuracy = 31.26%: 100%|██████████| 572/572 [00:05<00:00, 111.52it/s]\n",
            "[13 / 50]   Val: Loss = 0.21445, Accuracy = 18.92%: 100%|██████████| 13/13 [00:00<00:00, 82.77it/s]\n",
            "[14 / 50] Train: Loss = 0.01694, Accuracy = 31.25%: 100%|██████████| 572/572 [00:05<00:00, 111.10it/s]\n",
            "[14 / 50]   Val: Loss = 0.22741, Accuracy = 18.23%: 100%|██████████| 13/13 [00:00<00:00, 79.23it/s]\n",
            "[15 / 50] Train: Loss = 0.01392, Accuracy = 31.09%: 100%|██████████| 572/572 [00:05<00:00, 110.88it/s]\n",
            "[15 / 50]   Val: Loss = 0.23595, Accuracy = 18.51%: 100%|██████████| 13/13 [00:00<00:00, 77.52it/s]\n",
            "[16 / 50] Train: Loss = 0.01144, Accuracy = 31.21%: 100%|██████████| 572/572 [00:05<00:00, 107.28it/s]\n",
            "[16 / 50]   Val: Loss = 0.24601, Accuracy = 18.02%: 100%|██████████| 13/13 [00:00<00:00, 80.94it/s]\n",
            "[17 / 50] Train: Loss = 0.00958, Accuracy = 31.20%: 100%|██████████| 572/572 [00:05<00:00, 109.98it/s]\n",
            "[17 / 50]   Val: Loss = 0.25980, Accuracy = 18.16%: 100%|██████████| 13/13 [00:00<00:00, 80.39it/s]\n",
            "[18 / 50] Train: Loss = 0.00812, Accuracy = 31.38%: 100%|██████████| 572/572 [00:05<00:00, 111.51it/s]\n",
            "[18 / 50]   Val: Loss = 0.26438, Accuracy = 18.81%: 100%|██████████| 13/13 [00:00<00:00, 79.36it/s]\n",
            "[19 / 50] Train: Loss = 0.00695, Accuracy = 31.41%: 100%|██████████| 572/572 [00:05<00:00, 109.63it/s]\n",
            "[19 / 50]   Val: Loss = 0.27904, Accuracy = 18.10%: 100%|██████████| 13/13 [00:00<00:00, 79.47it/s]\n",
            "[20 / 50] Train: Loss = 0.00650, Accuracy = 31.32%: 100%|██████████| 572/572 [00:05<00:00, 110.16it/s]\n",
            "[20 / 50]   Val: Loss = 0.28916, Accuracy = 18.48%: 100%|██████████| 13/13 [00:00<00:00, 83.67it/s]\n",
            "[21 / 50] Train: Loss = 0.00612, Accuracy = 31.24%: 100%|██████████| 572/572 [00:05<00:00, 106.86it/s]\n",
            "[21 / 50]   Val: Loss = 0.28954, Accuracy = 18.97%: 100%|██████████| 13/13 [00:00<00:00, 80.05it/s]\n",
            "[22 / 50] Train: Loss = 0.00618, Accuracy = 31.52%: 100%|██████████| 572/572 [00:05<00:00, 108.04it/s]\n",
            "[22 / 50]   Val: Loss = 0.30066, Accuracy = 17.85%: 100%|██████████| 13/13 [00:00<00:00, 74.82it/s]\n",
            "[23 / 50] Train: Loss = 0.00568, Accuracy = 31.33%: 100%|██████████| 572/572 [00:05<00:00, 106.74it/s]\n",
            "[23 / 50]   Val: Loss = 0.30945, Accuracy = 18.17%: 100%|██████████| 13/13 [00:00<00:00, 75.72it/s]\n",
            "[24 / 50] Train: Loss = 0.00518, Accuracy = 31.43%: 100%|██████████| 572/572 [00:05<00:00, 108.73it/s]\n",
            "[24 / 50]   Val: Loss = 0.31358, Accuracy = 19.41%: 100%|██████████| 13/13 [00:00<00:00, 74.81it/s]\n",
            "[25 / 50] Train: Loss = 0.00517, Accuracy = 31.50%: 100%|██████████| 572/572 [00:05<00:00, 107.52it/s]\n",
            "[25 / 50]   Val: Loss = 0.30618, Accuracy = 18.49%: 100%|██████████| 13/13 [00:00<00:00, 78.21it/s]\n",
            "[26 / 50] Train: Loss = 0.00495, Accuracy = 31.33%: 100%|██████████| 572/572 [00:05<00:00, 104.09it/s]\n",
            "[26 / 50]   Val: Loss = 0.32531, Accuracy = 18.92%: 100%|██████████| 13/13 [00:00<00:00, 78.57it/s]\n",
            "[27 / 50] Train: Loss = 0.00492, Accuracy = 31.08%: 100%|██████████| 572/572 [00:05<00:00, 106.71it/s]\n",
            "[27 / 50]   Val: Loss = 0.32879, Accuracy = 18.79%: 100%|██████████| 13/13 [00:00<00:00, 83.80it/s]\n",
            "[28 / 50] Train: Loss = 0.00484, Accuracy = 31.19%: 100%|██████████| 572/572 [00:05<00:00, 108.08it/s]\n",
            "[28 / 50]   Val: Loss = 0.32476, Accuracy = 17.65%: 100%|██████████| 13/13 [00:00<00:00, 78.61it/s]\n",
            "[29 / 50] Train: Loss = 0.00490, Accuracy = 31.14%: 100%|██████████| 572/572 [00:05<00:00, 106.40it/s]\n",
            "[29 / 50]   Val: Loss = 0.32839, Accuracy = 18.07%: 100%|██████████| 13/13 [00:00<00:00, 73.17it/s]\n",
            "[30 / 50] Train: Loss = 0.00488, Accuracy = 31.37%: 100%|██████████| 572/572 [00:05<00:00, 102.28it/s]\n",
            "[30 / 50]   Val: Loss = 0.32768, Accuracy = 18.32%: 100%|██████████| 13/13 [00:00<00:00, 78.53it/s]\n",
            "[31 / 50] Train: Loss = 0.00459, Accuracy = 31.19%: 100%|██████████| 572/572 [00:05<00:00, 105.19it/s]\n",
            "[31 / 50]   Val: Loss = 0.33892, Accuracy = 18.47%: 100%|██████████| 13/13 [00:00<00:00, 74.82it/s]\n",
            "[32 / 50] Train: Loss = 0.00439, Accuracy = 31.47%: 100%|██████████| 572/572 [00:05<00:00, 104.38it/s]\n",
            "[32 / 50]   Val: Loss = 0.34933, Accuracy = 17.98%: 100%|██████████| 13/13 [00:00<00:00, 76.92it/s]\n",
            "[33 / 50] Train: Loss = 0.00421, Accuracy = 31.24%: 100%|██████████| 572/572 [00:05<00:00, 101.96it/s]\n",
            "[33 / 50]   Val: Loss = 0.34297, Accuracy = 18.65%: 100%|██████████| 13/13 [00:00<00:00, 80.81it/s]\n",
            "[34 / 50] Train: Loss = 0.00423, Accuracy = 31.47%: 100%|██████████| 572/572 [00:05<00:00, 105.90it/s]\n",
            "[34 / 50]   Val: Loss = 0.35050, Accuracy = 18.14%: 100%|██████████| 13/13 [00:00<00:00, 72.66it/s]\n",
            "[35 / 50] Train: Loss = 0.00452, Accuracy = 31.34%: 100%|██████████| 572/572 [00:05<00:00, 103.91it/s]\n",
            "[35 / 50]   Val: Loss = 0.34908, Accuracy = 19.63%: 100%|██████████| 13/13 [00:00<00:00, 78.43it/s]\n",
            "[36 / 50] Train: Loss = 0.00539, Accuracy = 31.47%: 100%|██████████| 572/572 [00:05<00:00, 105.95it/s]\n",
            "[36 / 50]   Val: Loss = 0.35323, Accuracy = 18.98%: 100%|██████████| 13/13 [00:00<00:00, 78.82it/s]\n",
            "[37 / 50] Train: Loss = 0.00456, Accuracy = 31.48%: 100%|██████████| 572/572 [00:05<00:00, 105.40it/s]\n",
            "[37 / 50]   Val: Loss = 0.36051, Accuracy = 19.17%: 100%|██████████| 13/13 [00:00<00:00, 77.67it/s]\n",
            "[38 / 50] Train: Loss = 0.00405, Accuracy = 31.33%: 100%|██████████| 572/572 [00:05<00:00, 107.43it/s]\n",
            "[38 / 50]   Val: Loss = 0.35570, Accuracy = 19.11%: 100%|██████████| 13/13 [00:00<00:00, 80.21it/s]\n",
            "[39 / 50] Train: Loss = 0.00387, Accuracy = 31.18%: 100%|██████████| 572/572 [00:05<00:00, 104.06it/s]\n",
            "[39 / 50]   Val: Loss = 0.36035, Accuracy = 18.84%: 100%|██████████| 13/13 [00:00<00:00, 78.86it/s]\n",
            "[40 / 50] Train: Loss = 0.00393, Accuracy = 31.31%: 100%|██████████| 572/572 [00:05<00:00, 105.39it/s]\n",
            "[40 / 50]   Val: Loss = 0.35854, Accuracy = 19.77%: 100%|██████████| 13/13 [00:00<00:00, 83.85it/s]\n",
            "[41 / 50] Train: Loss = 0.00396, Accuracy = 31.45%: 100%|██████████| 572/572 [00:05<00:00, 106.84it/s]\n",
            "[41 / 50]   Val: Loss = 0.36586, Accuracy = 17.72%: 100%|██████████| 13/13 [00:00<00:00, 76.35it/s]\n",
            "[42 / 50] Train: Loss = 0.00424, Accuracy = 31.58%: 100%|██████████| 572/572 [00:05<00:00, 103.70it/s]\n",
            "[42 / 50]   Val: Loss = 0.36188, Accuracy = 17.95%: 100%|██████████| 13/13 [00:00<00:00, 78.19it/s]\n",
            "[43 / 50] Train: Loss = 0.00614, Accuracy = 31.25%: 100%|██████████| 572/572 [00:05<00:00, 106.09it/s]\n",
            "[43 / 50]   Val: Loss = 0.36602, Accuracy = 18.32%: 100%|██████████| 13/13 [00:00<00:00, 74.71it/s]\n",
            "[44 / 50] Train: Loss = 0.00414, Accuracy = 31.29%: 100%|██████████| 572/572 [00:05<00:00, 106.11it/s]\n",
            "[44 / 50]   Val: Loss = 0.36957, Accuracy = 17.29%: 100%|██████████| 13/13 [00:00<00:00, 70.71it/s]\n",
            "[45 / 50] Train: Loss = 0.00374, Accuracy = 31.38%: 100%|██████████| 572/572 [00:05<00:00, 105.99it/s]\n",
            "[45 / 50]   Val: Loss = 0.37365, Accuracy = 18.70%: 100%|██████████| 13/13 [00:00<00:00, 77.17it/s]\n",
            "[46 / 50] Train: Loss = 0.00371, Accuracy = 31.27%: 100%|██████████| 572/572 [00:05<00:00, 103.57it/s]\n",
            "[46 / 50]   Val: Loss = 0.37026, Accuracy = 17.94%: 100%|██████████| 13/13 [00:00<00:00, 72.89it/s]\n",
            "[47 / 50] Train: Loss = 0.00375, Accuracy = 31.27%: 100%|██████████| 572/572 [00:05<00:00, 102.83it/s]\n",
            "[47 / 50]   Val: Loss = 0.36966, Accuracy = 17.86%: 100%|██████████| 13/13 [00:00<00:00, 79.23it/s]\n",
            "[48 / 50] Train: Loss = 0.00380, Accuracy = 31.36%: 100%|██████████| 572/572 [00:05<00:00, 105.26it/s]\n",
            "[48 / 50]   Val: Loss = 0.36660, Accuracy = 18.80%: 100%|██████████| 13/13 [00:00<00:00, 80.08it/s]\n",
            "[49 / 50] Train: Loss = 0.00377, Accuracy = 31.31%: 100%|██████████| 572/572 [00:05<00:00, 103.35it/s]\n",
            "[49 / 50]   Val: Loss = 0.35970, Accuracy = 19.12%: 100%|██████████| 13/13 [00:00<00:00, 79.10it/s]\n",
            "[50 / 50] Train: Loss = 0.00513, Accuracy = 31.29%: 100%|██████████| 572/572 [00:05<00:00, 105.78it/s]\n",
            "[50 / 50]   Val: Loss = 0.37056, Accuracy = 18.09%: 100%|██████████| 13/13 [00:00<00:00, 78.08it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98wr38_rw55D"
      },
      "source": [
        "logits = model(X_batch)\n",
        "argmax_logits = logits.view(-1, tagset_size).argmax(dim=1)\n",
        "accuracy = (argmax_logits == y_batch.view(-1)).sum() / y_batch.numel()\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWL8lOF4MSZd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZpY_Q1xZ18h"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsCstxiO03oT"
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxaRBpQd0pat"
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        <create me>\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        <use me>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBtI6BDE-Fc7"
      },
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=embeddings,\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPUuAPGhEGVR"
      },
      "source": [
        "<calc test accuracy>"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}