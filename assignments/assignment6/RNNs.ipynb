{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Week 06 - RNNs, part 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii-fGrc3MSZO"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P59NYU98GCb9"
      },
      "source": [
        "!pip3 -qq install torch\n",
        "!pip3 -qq install bokeh\n",
        "!pip3 -qq install gensim\n",
        "!pip3 -qq install nltk"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sVtGHmA9aBM"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiA2dGmgF1rW",
        "outputId": "18a5cc31-637d-4746-ad5b-ecf8645ab642"
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QstS4NO0L97c",
        "outputId": "901f16c7-7cfd-4c33-cc2f-310ffcd94935"
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTai8Ta0lgwL",
        "outputId": "5e1e947f-1b33-4a51-b283-f742fbbcfc45"
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCjwwDs6Zq9x",
        "outputId": "073f406f-2be4-4a72-bf7e-f9f3b7dcb6be"
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'NUM', 'DET', '.', 'VERB', 'NOUN', 'PRON', 'X', 'ADJ', 'ADV', 'CONJ', 'ADP', 'PRT'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "URC1B2nvPGFt",
        "outputId": "1d9bd201-a13c-4a30-a2b3-3c02d30e0cd7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rWmSToIaeAo",
        "outputId": "dc15f14a-de6c-46a9-9d59-fbb845419b17"
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjz_Rk0bbMyH",
        "outputId": "00c9f91a-18c5-4716-fc9d-c91cc26c1f6b"
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XCuxEBVbOY_",
        "outputId": "edc0cf00-b359-40d5-b19a-3d34cbddaa91"
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtRbz1SwgEqc"
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhsTKZalfih6"
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4XsRII5kW5x",
        "outputId": "665b40a3-46d3-4722-ea52-9ca3baa1793c"
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVEHju54d68T"
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1, bidirectional=False):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(input_size=word_emb_dim,\n",
        "                            hidden_size=lstm_hidden_dim,\n",
        "                            num_layers=lstm_layers_count,\n",
        "                            bidirectional=bidirectional)\n",
        "        \n",
        "        if bidirectional:\n",
        "            lstm_hidden_dim_output = 2 * lstm_hidden_dim\n",
        "        else:\n",
        "            lstm_hidden_dim_output = lstm_hidden_dim\n",
        "\n",
        "        self.fc = nn.Linear(lstm_hidden_dim_output, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        output = self.embedding(inputs)\n",
        "        output, _ = self.lstm(output)\n",
        "        output = self.fc(output)\n",
        "        return output"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoR4CSh1Eyxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41863cfa-1197-491c-c23e-636b93e778e8"
      },
      "source": [
        "rnn = nn.LSTM(10, 20, 2) # (input_size, hidden_size, num_layers)\n",
        "input = torch.randn(5, 3, 10) # (seq_len, batch, input_size)\n",
        "h0 = torch.randn(2, 3, 20)\n",
        "c0 = torch.randn(2, 3, 20)\n",
        "output, (hn, cn) = rnn(input, (h0, c0)) \n",
        "# output = (seq_len, batch, num_directions * hidden_size)\n",
        "# hn = (num_layers * num_directions, batch, hidden_size)\n",
        "# cn = (num_layers * num_directions, batch, hidden_size)\n",
        "print(output.shape, hn.shape, cn.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 3, 20]) torch.Size([2, 3, 20]) torch.Size([2, 3, 20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbrxsZ2mehWB",
        "outputId": "9c2bc24b-a448-4a14-e143-238cb3ab431e"
      },
      "source": [
        "tagset_size = len(tag2ind)  # n_classes\n",
        "\n",
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "argmax_logits = logits.view(-1, tagset_size).argmax(dim=1)\n",
        "accuracy = (argmax_logits == y_batch.view(-1)).sum() / y_batch.numel()\n",
        "print(accuracy)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0859)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMUyUm1hgpe3",
        "outputId": "1a30cf3b-e172-4859-debb-49d90e7d043f"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion(logits.view(-1, 13), y_batch.view(-1))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.5884, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FprPQ0gllo7b"
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "                \n",
        "                y_batch = y_batch.view(-1)\n",
        "                logits = logits.view(-1, tagset_size)\n",
        "\n",
        "                loss = criterion(logits, y_batch)\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                logits_argmax = logits.argmax(dim=1)\n",
        "                y_batch_not_zeros_mask = (y_batch != 0).int()\n",
        "                y_batch_count_zeros = (y_batch_not_zeros_mask == 0).sum()\n",
        "    \n",
        "                cur_correct_count, cur_sum_count = (logits_argmax * y_batch_not_zeros_mask == y_batch).sum(), y_batch.numel()\n",
        "                cur_correct_count, cur_sum_count = cur_correct_count - y_batch_count_zeros, cur_sum_count - y_batch_count_zeros\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "                \n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        " \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')\n",
        "        "
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqfbeh1ltEYa",
        "outputId": "20431085-eee7-40a8-83ce-d5e239951337"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 1.69207, Accuracy = 50.25%: 100%|██████████| 572/572 [00:06<00:00, 93.11it/s]\n",
            "[1 / 50]   Val: Loss = 1.12044, Accuracy = 65.97%: 100%|██████████| 13/13 [00:00<00:00, 61.41it/s]\n",
            "[2 / 50] Train: Loss = 0.92530, Accuracy = 71.06%: 100%|██████████| 572/572 [00:06<00:00, 94.10it/s]\n",
            "[2 / 50]   Val: Loss = 0.78870, Accuracy = 74.46%: 100%|██████████| 13/13 [00:00<00:00, 66.22it/s]\n",
            "[3 / 50] Train: Loss = 0.70858, Accuracy = 77.04%: 100%|██████████| 572/572 [00:06<00:00, 93.24it/s]\n",
            "[3 / 50]   Val: Loss = 0.64848, Accuracy = 78.57%: 100%|██████████| 13/13 [00:00<00:00, 64.91it/s]\n",
            "[4 / 50] Train: Loss = 0.59981, Accuracy = 80.26%: 100%|██████████| 572/572 [00:06<00:00, 93.38it/s]\n",
            "[4 / 50]   Val: Loss = 0.56650, Accuracy = 80.95%: 100%|██████████| 13/13 [00:00<00:00, 66.34it/s]\n",
            "[5 / 50] Train: Loss = 0.52864, Accuracy = 82.39%: 100%|██████████| 572/572 [00:06<00:00, 93.08it/s]\n",
            "[5 / 50]   Val: Loss = 0.50885, Accuracy = 82.68%: 100%|██████████| 13/13 [00:00<00:00, 62.87it/s]\n",
            "[6 / 50] Train: Loss = 0.47501, Accuracy = 84.20%: 100%|██████████| 572/572 [00:06<00:00, 93.34it/s]\n",
            "[6 / 50]   Val: Loss = 0.46348, Accuracy = 84.09%: 100%|██████████| 13/13 [00:00<00:00, 63.06it/s]\n",
            "[7 / 50] Train: Loss = 0.43246, Accuracy = 85.64%: 100%|██████████| 572/572 [00:06<00:00, 93.44it/s]\n",
            "[7 / 50]   Val: Loss = 0.42713, Accuracy = 85.20%: 100%|██████████| 13/13 [00:00<00:00, 63.77it/s]\n",
            "[8 / 50] Train: Loss = 0.39699, Accuracy = 86.79%: 100%|██████████| 572/572 [00:06<00:00, 92.30it/s]\n",
            "[8 / 50]   Val: Loss = 0.39752, Accuracy = 86.15%: 100%|██████████| 13/13 [00:00<00:00, 65.43it/s]\n",
            "[9 / 50] Train: Loss = 0.36763, Accuracy = 87.78%: 100%|██████████| 572/572 [00:06<00:00, 92.16it/s]\n",
            "[9 / 50]   Val: Loss = 0.37325, Accuracy = 86.92%: 100%|██████████| 13/13 [00:00<00:00, 64.87it/s]\n",
            "[10 / 50] Train: Loss = 0.34238, Accuracy = 88.61%: 100%|██████████| 572/572 [00:06<00:00, 91.62it/s]\n",
            "[10 / 50]   Val: Loss = 0.35185, Accuracy = 87.58%: 100%|██████████| 13/13 [00:00<00:00, 60.90it/s]\n",
            "[11 / 50] Train: Loss = 0.32002, Accuracy = 89.34%: 100%|██████████| 572/572 [00:06<00:00, 92.24it/s]\n",
            "[11 / 50]   Val: Loss = 0.33556, Accuracy = 88.16%: 100%|██████████| 13/13 [00:00<00:00, 68.24it/s]\n",
            "[12 / 50] Train: Loss = 0.30082, Accuracy = 89.98%: 100%|██████████| 572/572 [00:06<00:00, 92.77it/s]\n",
            "[12 / 50]   Val: Loss = 0.32095, Accuracy = 88.67%: 100%|██████████| 13/13 [00:00<00:00, 59.11it/s]\n",
            "[13 / 50] Train: Loss = 0.28346, Accuracy = 90.58%: 100%|██████████| 572/572 [00:06<00:00, 92.01it/s]\n",
            "[13 / 50]   Val: Loss = 0.30465, Accuracy = 89.19%: 100%|██████████| 13/13 [00:00<00:00, 66.10it/s]\n",
            "[14 / 50] Train: Loss = 0.26782, Accuracy = 91.11%: 100%|██████████| 572/572 [00:06<00:00, 92.38it/s]\n",
            "[14 / 50]   Val: Loss = 0.29300, Accuracy = 89.63%: 100%|██████████| 13/13 [00:00<00:00, 65.99it/s]\n",
            "[15 / 50] Train: Loss = 0.25392, Accuracy = 91.60%: 100%|██████████| 572/572 [00:06<00:00, 91.42it/s]\n",
            "[15 / 50]   Val: Loss = 0.28114, Accuracy = 90.04%: 100%|██████████| 13/13 [00:00<00:00, 67.04it/s]\n",
            "[16 / 50] Train: Loss = 0.24124, Accuracy = 92.03%: 100%|██████████| 572/572 [00:06<00:00, 90.91it/s]\n",
            "[16 / 50]   Val: Loss = 0.27147, Accuracy = 90.36%: 100%|██████████| 13/13 [00:00<00:00, 62.01it/s]\n",
            "[17 / 50] Train: Loss = 0.22954, Accuracy = 92.42%: 100%|██████████| 572/572 [00:06<00:00, 91.02it/s]\n",
            "[17 / 50]   Val: Loss = 0.26313, Accuracy = 90.64%: 100%|██████████| 13/13 [00:00<00:00, 65.13it/s]\n",
            "[18 / 50] Train: Loss = 0.21886, Accuracy = 92.77%: 100%|██████████| 572/572 [00:06<00:00, 90.44it/s]\n",
            "[18 / 50]   Val: Loss = 0.25450, Accuracy = 90.96%: 100%|██████████| 13/13 [00:00<00:00, 65.58it/s]\n",
            "[19 / 50] Train: Loss = 0.20927, Accuracy = 93.10%: 100%|██████████| 572/572 [00:06<00:00, 90.14it/s]\n",
            "[19 / 50]   Val: Loss = 0.24675, Accuracy = 91.25%: 100%|██████████| 13/13 [00:00<00:00, 61.08it/s]\n",
            "[20 / 50] Train: Loss = 0.20021, Accuracy = 93.39%: 100%|██████████| 572/572 [00:06<00:00, 90.63it/s]\n",
            "[20 / 50]   Val: Loss = 0.24033, Accuracy = 91.45%: 100%|██████████| 13/13 [00:00<00:00, 61.38it/s]\n",
            "[21 / 50] Train: Loss = 0.19169, Accuracy = 93.68%: 100%|██████████| 572/572 [00:06<00:00, 91.26it/s]\n",
            "[21 / 50]   Val: Loss = 0.23501, Accuracy = 91.65%: 100%|██████████| 13/13 [00:00<00:00, 60.86it/s]\n",
            "[22 / 50] Train: Loss = 0.18389, Accuracy = 93.92%: 100%|██████████| 572/572 [00:06<00:00, 90.91it/s]\n",
            "[22 / 50]   Val: Loss = 0.23089, Accuracy = 91.87%: 100%|██████████| 13/13 [00:00<00:00, 66.64it/s]\n",
            "[23 / 50] Train: Loss = 0.17661, Accuracy = 94.17%: 100%|██████████| 572/572 [00:06<00:00, 90.57it/s]\n",
            "[23 / 50]   Val: Loss = 0.22584, Accuracy = 92.00%: 100%|██████████| 13/13 [00:00<00:00, 64.64it/s]\n",
            "[24 / 50] Train: Loss = 0.16999, Accuracy = 94.39%: 100%|██████████| 572/572 [00:06<00:00, 90.38it/s]\n",
            "[24 / 50]   Val: Loss = 0.22280, Accuracy = 92.15%: 100%|██████████| 13/13 [00:00<00:00, 62.66it/s]\n",
            "[25 / 50] Train: Loss = 0.16358, Accuracy = 94.60%: 100%|██████████| 572/572 [00:06<00:00, 91.00it/s]\n",
            "[25 / 50]   Val: Loss = 0.21696, Accuracy = 92.30%: 100%|██████████| 13/13 [00:00<00:00, 63.86it/s]\n",
            "[26 / 50] Train: Loss = 0.15757, Accuracy = 94.79%: 100%|██████████| 572/572 [00:06<00:00, 90.73it/s]\n",
            "[26 / 50]   Val: Loss = 0.21312, Accuracy = 92.48%: 100%|██████████| 13/13 [00:00<00:00, 62.75it/s]\n",
            "[27 / 50] Train: Loss = 0.15179, Accuracy = 94.98%: 100%|██████████| 572/572 [00:06<00:00, 89.92it/s]\n",
            "[27 / 50]   Val: Loss = 0.21060, Accuracy = 92.58%: 100%|██████████| 13/13 [00:00<00:00, 65.23it/s]\n",
            "[28 / 50] Train: Loss = 0.14640, Accuracy = 95.17%: 100%|██████████| 572/572 [00:06<00:00, 90.06it/s]\n",
            "[28 / 50]   Val: Loss = 0.20706, Accuracy = 92.70%: 100%|██████████| 13/13 [00:00<00:00, 62.42it/s]\n",
            "[29 / 50] Train: Loss = 0.14116, Accuracy = 95.33%: 100%|██████████| 572/572 [00:06<00:00, 90.61it/s]\n",
            "[29 / 50]   Val: Loss = 0.20490, Accuracy = 92.81%: 100%|██████████| 13/13 [00:00<00:00, 58.05it/s]\n",
            "[30 / 50] Train: Loss = 0.13646, Accuracy = 95.50%: 100%|██████████| 572/572 [00:06<00:00, 89.71it/s]\n",
            "[30 / 50]   Val: Loss = 0.19989, Accuracy = 92.94%: 100%|██████████| 13/13 [00:00<00:00, 61.32it/s]\n",
            "[31 / 50] Train: Loss = 0.13184, Accuracy = 95.66%: 100%|██████████| 572/572 [00:06<00:00, 90.04it/s]\n",
            "[31 / 50]   Val: Loss = 0.19856, Accuracy = 92.97%: 100%|██████████| 13/13 [00:00<00:00, 64.56it/s]\n",
            "[32 / 50] Train: Loss = 0.12759, Accuracy = 95.80%: 100%|██████████| 572/572 [00:06<00:00, 89.35it/s]\n",
            "[32 / 50]   Val: Loss = 0.19637, Accuracy = 93.09%: 100%|██████████| 13/13 [00:00<00:00, 60.38it/s]\n",
            "[33 / 50] Train: Loss = 0.12360, Accuracy = 95.94%: 100%|██████████| 572/572 [00:06<00:00, 90.06it/s]\n",
            "[33 / 50]   Val: Loss = 0.19415, Accuracy = 93.17%: 100%|██████████| 13/13 [00:00<00:00, 63.85it/s]\n",
            "[34 / 50] Train: Loss = 0.11929, Accuracy = 96.08%: 100%|██████████| 572/572 [00:06<00:00, 89.59it/s]\n",
            "[34 / 50]   Val: Loss = 0.19215, Accuracy = 93.26%: 100%|██████████| 13/13 [00:00<00:00, 59.05it/s]\n",
            "[35 / 50] Train: Loss = 0.11548, Accuracy = 96.21%: 100%|██████████| 572/572 [00:06<00:00, 89.40it/s]\n",
            "[35 / 50]   Val: Loss = 0.18900, Accuracy = 93.34%: 100%|██████████| 13/13 [00:00<00:00, 59.93it/s]\n",
            "[36 / 50] Train: Loss = 0.11186, Accuracy = 96.34%: 100%|██████████| 572/572 [00:06<00:00, 89.21it/s]\n",
            "[36 / 50]   Val: Loss = 0.18819, Accuracy = 93.40%: 100%|██████████| 13/13 [00:00<00:00, 58.05it/s]\n",
            "[37 / 50] Train: Loss = 0.10818, Accuracy = 96.45%: 100%|██████████| 572/572 [00:06<00:00, 89.58it/s]\n",
            "[37 / 50]   Val: Loss = 0.18724, Accuracy = 93.43%: 100%|██████████| 13/13 [00:00<00:00, 62.73it/s]\n",
            "[38 / 50] Train: Loss = 0.10484, Accuracy = 96.56%: 100%|██████████| 572/572 [00:06<00:00, 89.56it/s]\n",
            "[38 / 50]   Val: Loss = 0.18714, Accuracy = 93.50%: 100%|██████████| 13/13 [00:00<00:00, 65.56it/s]\n",
            "[39 / 50] Train: Loss = 0.10140, Accuracy = 96.68%: 100%|██████████| 572/572 [00:06<00:00, 88.67it/s]\n",
            "[39 / 50]   Val: Loss = 0.18557, Accuracy = 93.56%: 100%|██████████| 13/13 [00:00<00:00, 61.10it/s]\n",
            "[40 / 50] Train: Loss = 0.09848, Accuracy = 96.78%: 100%|██████████| 572/572 [00:06<00:00, 88.90it/s]\n",
            "[40 / 50]   Val: Loss = 0.18421, Accuracy = 93.58%: 100%|██████████| 13/13 [00:00<00:00, 64.09it/s]\n",
            "[41 / 50] Train: Loss = 0.09546, Accuracy = 96.88%: 100%|██████████| 572/572 [00:06<00:00, 89.04it/s]\n",
            "[41 / 50]   Val: Loss = 0.18245, Accuracy = 93.66%: 100%|██████████| 13/13 [00:00<00:00, 63.45it/s]\n",
            "[42 / 50] Train: Loss = 0.09237, Accuracy = 96.99%: 100%|██████████| 572/572 [00:06<00:00, 89.35it/s]\n",
            "[42 / 50]   Val: Loss = 0.18497, Accuracy = 93.72%: 100%|██████████| 13/13 [00:00<00:00, 60.97it/s]\n",
            "[43 / 50] Train: Loss = 0.08973, Accuracy = 97.08%: 100%|██████████| 572/572 [00:06<00:00, 87.98it/s]\n",
            "[43 / 50]   Val: Loss = 0.18219, Accuracy = 93.76%: 100%|██████████| 13/13 [00:00<00:00, 60.66it/s]\n",
            "[44 / 50] Train: Loss = 0.08711, Accuracy = 97.18%: 100%|██████████| 572/572 [00:06<00:00, 88.25it/s]\n",
            "[44 / 50]   Val: Loss = 0.18361, Accuracy = 93.81%: 100%|██████████| 13/13 [00:00<00:00, 62.73it/s]\n",
            "[45 / 50] Train: Loss = 0.08426, Accuracy = 97.26%: 100%|██████████| 572/572 [00:06<00:00, 88.56it/s]\n",
            "[45 / 50]   Val: Loss = 0.18125, Accuracy = 93.83%: 100%|██████████| 13/13 [00:00<00:00, 61.89it/s]\n",
            "[46 / 50] Train: Loss = 0.08166, Accuracy = 97.35%: 100%|██████████| 572/572 [00:06<00:00, 87.45it/s]\n",
            "[46 / 50]   Val: Loss = 0.18355, Accuracy = 93.88%: 100%|██████████| 13/13 [00:00<00:00, 60.00it/s]\n",
            "[47 / 50] Train: Loss = 0.07922, Accuracy = 97.44%: 100%|██████████| 572/572 [00:06<00:00, 88.01it/s]\n",
            "[47 / 50]   Val: Loss = 0.18356, Accuracy = 93.91%: 100%|██████████| 13/13 [00:00<00:00, 59.94it/s]\n",
            "[48 / 50] Train: Loss = 0.07673, Accuracy = 97.52%: 100%|██████████| 572/572 [00:06<00:00, 88.02it/s]\n",
            "[48 / 50]   Val: Loss = 0.18008, Accuracy = 93.94%: 100%|██████████| 13/13 [00:00<00:00, 61.51it/s]\n",
            "[49 / 50] Train: Loss = 0.07450, Accuracy = 97.60%: 100%|██████████| 572/572 [00:06<00:00, 87.60it/s]\n",
            "[49 / 50]   Val: Loss = 0.18298, Accuracy = 93.97%: 100%|██████████| 13/13 [00:00<00:00, 61.23it/s]\n",
            "[50 / 50] Train: Loss = 0.07231, Accuracy = 97.67%: 100%|██████████| 572/572 [00:06<00:00, 87.72it/s]\n",
            "[50 / 50]   Val: Loss = 0.18115, Accuracy = 93.97%: 100%|██████████| 13/13 [00:00<00:00, 63.54it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaDvF3rcgP2G",
        "outputId": "51cbc759-41c3-4c0a-efae-031b46a619a7"
      },
      "source": [
        "loss, accuracy = do_epoch(model, criterion, (X_test, y_test), 64, optimizer=None, name='test')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " test Loss = 0.17842, Accuracy = 94.09%: 100%|██████████| 224/224 [00:00<00:00, 230.11it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWL8lOF4MSZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3ccd181-6332-4ba5-8238-5327578e7938"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind),\n",
        "    bidirectional=True\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 1.45499, Accuracy = 57.36%: 100%|██████████| 572/572 [00:07<00:00, 77.01it/s]\n",
            "[1 / 50]   Val: Loss = 0.88260, Accuracy = 72.57%: 100%|██████████| 13/13 [00:00<00:00, 44.95it/s]\n",
            "[2 / 50] Train: Loss = 0.71474, Accuracy = 77.69%: 100%|██████████| 572/572 [00:07<00:00, 76.43it/s]\n",
            "[2 / 50]   Val: Loss = 0.60783, Accuracy = 80.93%: 100%|██████████| 13/13 [00:00<00:00, 46.83it/s]\n",
            "[3 / 50] Train: Loss = 0.54352, Accuracy = 82.71%: 100%|██████████| 572/572 [00:07<00:00, 76.07it/s]\n",
            "[3 / 50]   Val: Loss = 0.50101, Accuracy = 83.96%: 100%|██████████| 13/13 [00:00<00:00, 49.01it/s]\n",
            "[4 / 50] Train: Loss = 0.45743, Accuracy = 85.23%: 100%|██████████| 572/572 [00:07<00:00, 77.02it/s]\n",
            "[4 / 50]   Val: Loss = 0.43537, Accuracy = 86.01%: 100%|██████████| 13/13 [00:00<00:00, 47.24it/s]\n",
            "[5 / 50] Train: Loss = 0.40025, Accuracy = 87.02%: 100%|██████████| 572/572 [00:07<00:00, 77.14it/s]\n",
            "[5 / 50]   Val: Loss = 0.38832, Accuracy = 87.44%: 100%|██████████| 13/13 [00:00<00:00, 48.01it/s]\n",
            "[6 / 50] Train: Loss = 0.35764, Accuracy = 88.38%: 100%|██████████| 572/572 [00:07<00:00, 76.41it/s]\n",
            "[6 / 50]   Val: Loss = 0.35276, Accuracy = 88.49%: 100%|██████████| 13/13 [00:00<00:00, 46.15it/s]\n",
            "[7 / 50] Train: Loss = 0.32355, Accuracy = 89.52%: 100%|██████████| 572/572 [00:07<00:00, 76.80it/s]\n",
            "[7 / 50]   Val: Loss = 0.32443, Accuracy = 89.47%: 100%|██████████| 13/13 [00:00<00:00, 47.65it/s]\n",
            "[8 / 50] Train: Loss = 0.29562, Accuracy = 90.47%: 100%|██████████| 572/572 [00:07<00:00, 76.14it/s]\n",
            "[8 / 50]   Val: Loss = 0.29986, Accuracy = 90.28%: 100%|██████████| 13/13 [00:00<00:00, 45.91it/s]\n",
            "[9 / 50] Train: Loss = 0.27220, Accuracy = 91.26%: 100%|██████████| 572/572 [00:07<00:00, 76.00it/s]\n",
            "[9 / 50]   Val: Loss = 0.27979, Accuracy = 90.97%: 100%|██████████| 13/13 [00:00<00:00, 50.06it/s]\n",
            "[10 / 50] Train: Loss = 0.25192, Accuracy = 91.96%: 100%|██████████| 572/572 [00:07<00:00, 76.36it/s]\n",
            "[10 / 50]   Val: Loss = 0.26334, Accuracy = 91.52%: 100%|██████████| 13/13 [00:00<00:00, 46.16it/s]\n",
            "[11 / 50] Train: Loss = 0.23402, Accuracy = 92.55%: 100%|██████████| 572/572 [00:07<00:00, 76.37it/s]\n",
            "[11 / 50]   Val: Loss = 0.24918, Accuracy = 92.03%: 100%|██████████| 13/13 [00:00<00:00, 50.29it/s]\n",
            "[12 / 50] Train: Loss = 0.21849, Accuracy = 93.09%: 100%|██████████| 572/572 [00:07<00:00, 76.27it/s]\n",
            "[12 / 50]   Val: Loss = 0.23689, Accuracy = 92.43%: 100%|██████████| 13/13 [00:00<00:00, 49.01it/s]\n",
            "[13 / 50] Train: Loss = 0.20442, Accuracy = 93.56%: 100%|██████████| 572/572 [00:07<00:00, 75.92it/s]\n",
            "[13 / 50]   Val: Loss = 0.22503, Accuracy = 92.84%: 100%|██████████| 13/13 [00:00<00:00, 46.36it/s]\n",
            "[14 / 50] Train: Loss = 0.19172, Accuracy = 93.97%: 100%|██████████| 572/572 [00:07<00:00, 75.73it/s]\n",
            "[14 / 50]   Val: Loss = 0.21518, Accuracy = 93.11%: 100%|██████████| 13/13 [00:00<00:00, 47.32it/s]\n",
            "[15 / 50] Train: Loss = 0.18035, Accuracy = 94.35%: 100%|██████████| 572/572 [00:07<00:00, 74.48it/s]\n",
            "[15 / 50]   Val: Loss = 0.20616, Accuracy = 93.41%: 100%|██████████| 13/13 [00:00<00:00, 45.75it/s]\n",
            "[16 / 50] Train: Loss = 0.17001, Accuracy = 94.69%: 100%|██████████| 572/572 [00:07<00:00, 75.51it/s]\n",
            "[16 / 50]   Val: Loss = 0.19770, Accuracy = 93.71%: 100%|██████████| 13/13 [00:00<00:00, 46.67it/s]\n",
            "[17 / 50] Train: Loss = 0.16059, Accuracy = 95.01%: 100%|██████████| 572/572 [00:07<00:00, 75.90it/s]\n",
            "[17 / 50]   Val: Loss = 0.19136, Accuracy = 93.92%: 100%|██████████| 13/13 [00:00<00:00, 49.12it/s]\n",
            "[18 / 50] Train: Loss = 0.15193, Accuracy = 95.30%: 100%|██████████| 572/572 [00:07<00:00, 75.37it/s]\n",
            "[18 / 50]   Val: Loss = 0.18448, Accuracy = 94.10%: 100%|██████████| 13/13 [00:00<00:00, 48.72it/s]\n",
            "[19 / 50] Train: Loss = 0.14360, Accuracy = 95.57%: 100%|██████████| 572/572 [00:07<00:00, 75.92it/s]\n",
            "[19 / 50]   Val: Loss = 0.17851, Accuracy = 94.33%: 100%|██████████| 13/13 [00:00<00:00, 48.25it/s]\n",
            "[20 / 50] Train: Loss = 0.13627, Accuracy = 95.81%: 100%|██████████| 572/572 [00:07<00:00, 75.40it/s]\n",
            "[20 / 50]   Val: Loss = 0.17467, Accuracy = 94.47%: 100%|██████████| 13/13 [00:00<00:00, 45.97it/s]\n",
            "[21 / 50] Train: Loss = 0.12922, Accuracy = 96.05%: 100%|██████████| 572/572 [00:07<00:00, 75.39it/s]\n",
            "[21 / 50]   Val: Loss = 0.16941, Accuracy = 94.60%: 100%|██████████| 13/13 [00:00<00:00, 47.31it/s]\n",
            "[22 / 50] Train: Loss = 0.12257, Accuracy = 96.26%: 100%|██████████| 572/572 [00:07<00:00, 75.41it/s]\n",
            "[22 / 50]   Val: Loss = 0.16433, Accuracy = 94.75%: 100%|██████████| 13/13 [00:00<00:00, 46.42it/s]\n",
            "[23 / 50] Train: Loss = 0.11673, Accuracy = 96.46%: 100%|██████████| 572/572 [00:07<00:00, 74.91it/s]\n",
            "[23 / 50]   Val: Loss = 0.16055, Accuracy = 94.87%: 100%|██████████| 13/13 [00:00<00:00, 48.96it/s]\n",
            "[24 / 50] Train: Loss = 0.11083, Accuracy = 96.63%: 100%|██████████| 572/572 [00:07<00:00, 75.10it/s]\n",
            "[24 / 50]   Val: Loss = 0.15719, Accuracy = 94.98%: 100%|██████████| 13/13 [00:00<00:00, 47.04it/s]\n",
            "[25 / 50] Train: Loss = 0.10540, Accuracy = 96.82%: 100%|██████████| 572/572 [00:07<00:00, 75.10it/s]\n",
            "[25 / 50]   Val: Loss = 0.15435, Accuracy = 95.07%: 100%|██████████| 13/13 [00:00<00:00, 46.43it/s]\n",
            "[26 / 50] Train: Loss = 0.10016, Accuracy = 96.98%: 100%|██████████| 572/572 [00:07<00:00, 75.27it/s]\n",
            "[26 / 50]   Val: Loss = 0.15134, Accuracy = 95.13%: 100%|██████████| 13/13 [00:00<00:00, 49.35it/s]\n",
            "[27 / 50] Train: Loss = 0.09530, Accuracy = 97.16%: 100%|██████████| 572/572 [00:07<00:00, 74.95it/s]\n",
            "[27 / 50]   Val: Loss = 0.14847, Accuracy = 95.23%: 100%|██████████| 13/13 [00:00<00:00, 47.58it/s]\n",
            "[28 / 50] Train: Loss = 0.09079, Accuracy = 97.30%: 100%|██████████| 572/572 [00:07<00:00, 74.14it/s]\n",
            "[28 / 50]   Val: Loss = 0.14714, Accuracy = 95.27%: 100%|██████████| 13/13 [00:00<00:00, 46.88it/s]\n",
            "[29 / 50] Train: Loss = 0.08615, Accuracy = 97.46%: 100%|██████████| 572/572 [00:07<00:00, 74.42it/s]\n",
            "[29 / 50]   Val: Loss = 0.14268, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 47.39it/s]\n",
            "[30 / 50] Train: Loss = 0.08219, Accuracy = 97.59%: 100%|██████████| 572/572 [00:07<00:00, 74.56it/s]\n",
            "[30 / 50]   Val: Loss = 0.14138, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 48.84it/s]\n",
            "[31 / 50] Train: Loss = 0.07824, Accuracy = 97.71%: 100%|██████████| 572/572 [00:07<00:00, 74.50it/s]\n",
            "[31 / 50]   Val: Loss = 0.13942, Accuracy = 95.52%: 100%|██████████| 13/13 [00:00<00:00, 48.40it/s]\n",
            "[32 / 50] Train: Loss = 0.07432, Accuracy = 97.83%: 100%|██████████| 572/572 [00:07<00:00, 74.20it/s]\n",
            "[32 / 50]   Val: Loss = 0.13854, Accuracy = 95.56%: 100%|██████████| 13/13 [00:00<00:00, 45.00it/s]\n",
            "[33 / 50] Train: Loss = 0.07064, Accuracy = 97.95%: 100%|██████████| 572/572 [00:07<00:00, 73.97it/s]\n",
            "[33 / 50]   Val: Loss = 0.13670, Accuracy = 95.62%: 100%|██████████| 13/13 [00:00<00:00, 47.65it/s]\n",
            "[34 / 50] Train: Loss = 0.06728, Accuracy = 98.07%: 100%|██████████| 572/572 [00:07<00:00, 73.80it/s]\n",
            "[34 / 50]   Val: Loss = 0.13492, Accuracy = 95.68%: 100%|██████████| 13/13 [00:00<00:00, 45.52it/s]\n",
            "[35 / 50] Train: Loss = 0.06373, Accuracy = 98.18%: 100%|██████████| 572/572 [00:07<00:00, 73.35it/s]\n",
            "[35 / 50]   Val: Loss = 0.13551, Accuracy = 95.67%: 100%|██████████| 13/13 [00:00<00:00, 48.57it/s]\n",
            "[36 / 50] Train: Loss = 0.06054, Accuracy = 98.29%: 100%|██████████| 572/572 [00:07<00:00, 73.63it/s]\n",
            "[36 / 50]   Val: Loss = 0.13409, Accuracy = 95.73%: 100%|██████████| 13/13 [00:00<00:00, 45.25it/s]\n",
            "[37 / 50] Train: Loss = 0.05747, Accuracy = 98.38%: 100%|██████████| 572/572 [00:07<00:00, 73.95it/s]\n",
            "[37 / 50]   Val: Loss = 0.13596, Accuracy = 95.69%: 100%|██████████| 13/13 [00:00<00:00, 48.44it/s]\n",
            "[38 / 50] Train: Loss = 0.05447, Accuracy = 98.47%: 100%|██████████| 572/572 [00:07<00:00, 74.40it/s]\n",
            "[38 / 50]   Val: Loss = 0.13187, Accuracy = 95.83%: 100%|██████████| 13/13 [00:00<00:00, 48.44it/s]\n",
            "[39 / 50] Train: Loss = 0.05178, Accuracy = 98.57%: 100%|██████████| 572/572 [00:07<00:00, 74.20it/s]\n",
            "[39 / 50]   Val: Loss = 0.13265, Accuracy = 95.80%: 100%|██████████| 13/13 [00:00<00:00, 47.17it/s]\n",
            "[40 / 50] Train: Loss = 0.04893, Accuracy = 98.67%: 100%|██████████| 572/572 [00:07<00:00, 73.67it/s]\n",
            "[40 / 50]   Val: Loss = 0.13252, Accuracy = 95.85%: 100%|██████████| 13/13 [00:00<00:00, 46.05it/s]\n",
            "[41 / 50] Train: Loss = 0.04631, Accuracy = 98.74%: 100%|██████████| 572/572 [00:07<00:00, 72.64it/s]\n",
            "[41 / 50]   Val: Loss = 0.13277, Accuracy = 95.80%: 100%|██████████| 13/13 [00:00<00:00, 46.52it/s]\n",
            "[42 / 50] Train: Loss = 0.04372, Accuracy = 98.82%: 100%|██████████| 572/572 [00:07<00:00, 73.52it/s]\n",
            "[42 / 50]   Val: Loss = 0.13101, Accuracy = 95.89%: 100%|██████████| 13/13 [00:00<00:00, 49.06it/s]\n",
            "[43 / 50] Train: Loss = 0.04125, Accuracy = 98.90%: 100%|██████████| 572/572 [00:07<00:00, 73.19it/s]\n",
            "[43 / 50]   Val: Loss = 0.13182, Accuracy = 95.89%: 100%|██████████| 13/13 [00:00<00:00, 45.26it/s]\n",
            "[44 / 50] Train: Loss = 0.03901, Accuracy = 98.98%: 100%|██████████| 572/572 [00:07<00:00, 73.32it/s]\n",
            "[44 / 50]   Val: Loss = 0.13136, Accuracy = 95.91%: 100%|██████████| 13/13 [00:00<00:00, 45.85it/s]\n",
            "[45 / 50] Train: Loss = 0.03668, Accuracy = 99.04%: 100%|██████████| 572/572 [00:07<00:00, 73.11it/s]\n",
            "[45 / 50]   Val: Loss = 0.13143, Accuracy = 95.92%: 100%|██████████| 13/13 [00:00<00:00, 46.08it/s]\n",
            "[46 / 50] Train: Loss = 0.03465, Accuracy = 99.11%: 100%|██████████| 572/572 [00:07<00:00, 72.53it/s]\n",
            "[46 / 50]   Val: Loss = 0.13343, Accuracy = 95.88%: 100%|██████████| 13/13 [00:00<00:00, 48.42it/s]\n",
            "[47 / 50] Train: Loss = 0.03256, Accuracy = 99.18%: 100%|██████████| 572/572 [00:07<00:00, 72.44it/s]\n",
            "[47 / 50]   Val: Loss = 0.13302, Accuracy = 95.93%: 100%|██████████| 13/13 [00:00<00:00, 46.85it/s]\n",
            "[48 / 50] Train: Loss = 0.03052, Accuracy = 99.24%: 100%|██████████| 572/572 [00:07<00:00, 72.47it/s]\n",
            "[48 / 50]   Val: Loss = 0.13442, Accuracy = 95.92%: 100%|██████████| 13/13 [00:00<00:00, 47.29it/s]\n",
            "[49 / 50] Train: Loss = 0.02864, Accuracy = 99.30%: 100%|██████████| 572/572 [00:07<00:00, 72.80it/s]\n",
            "[49 / 50]   Val: Loss = 0.13486, Accuracy = 95.92%: 100%|██████████| 13/13 [00:00<00:00, 48.17it/s]\n",
            "[50 / 50] Train: Loss = 0.02683, Accuracy = 99.36%: 100%|██████████| 572/572 [00:07<00:00, 73.44it/s]\n",
            "[50 / 50]   Val: Loss = 0.13755, Accuracy = 95.85%: 100%|██████████| 13/13 [00:00<00:00, 46.75it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZpY_Q1xZ18h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb2e18c0-a32a-49d0-ed63-9c1c688d4050"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsCstxiO03oT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea13f275-04df-4e83-b477-d72326038c60"
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxaRBpQd0pat"
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=128, lstm_layers_count=1, bidirectional=False):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embeddings), freeze=False)\n",
        "        word_emb_dim = embeddings.shape[1]\n",
        "        self.lstm = nn.LSTM(input_size=word_emb_dim,\n",
        "                            hidden_size=lstm_hidden_dim,\n",
        "                            num_layers=lstm_layers_count,\n",
        "                            bidirectional=bidirectional)\n",
        "        \n",
        "        if bidirectional:\n",
        "            lstm_hidden_dim_output = 2 * lstm_hidden_dim\n",
        "        else:\n",
        "            lstm_hidden_dim_output = lstm_hidden_dim\n",
        "\n",
        "        self.fc = nn.Linear(lstm_hidden_dim_output, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        output = self.embedding(inputs)\n",
        "        output, _ = self.lstm(output)\n",
        "        output = self.fc(output)\n",
        "        return output"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBtI6BDE-Fc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea73210-e6b9-4152-eadb-c7e0e1a2b280"
      },
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=embeddings,\n",
        "    tagset_size=len(tag2ind),\n",
        "    bidirectional=True\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.33326, Accuracy = 90.39%: 100%|██████████| 572/572 [00:07<00:00, 77.35it/s]\n",
            "[1 / 50]   Val: Loss = 0.11153, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 46.59it/s]\n",
            "[2 / 50] Train: Loss = 0.06552, Accuracy = 97.96%: 100%|██████████| 572/572 [00:07<00:00, 78.44it/s]\n",
            "[2 / 50]   Val: Loss = 0.09832, Accuracy = 97.17%: 100%|██████████| 13/13 [00:00<00:00, 49.84it/s]\n",
            "[3 / 50] Train: Loss = 0.04188, Accuracy = 98.66%: 100%|██████████| 572/572 [00:07<00:00, 77.07it/s]\n",
            "[3 / 50]   Val: Loss = 0.08643, Accuracy = 97.36%: 100%|██████████| 13/13 [00:00<00:00, 48.60it/s]\n",
            "[4 / 50] Train: Loss = 0.03031, Accuracy = 99.02%: 100%|██████████| 572/572 [00:07<00:00, 77.84it/s]\n",
            "[4 / 50]   Val: Loss = 0.08987, Accuracy = 97.36%: 100%|██████████| 13/13 [00:00<00:00, 48.58it/s]\n",
            "[5 / 50] Train: Loss = 0.02303, Accuracy = 99.26%: 100%|██████████| 572/572 [00:07<00:00, 77.31it/s]\n",
            "[5 / 50]   Val: Loss = 0.09044, Accuracy = 97.45%: 100%|██████████| 13/13 [00:00<00:00, 49.20it/s]\n",
            "[6 / 50] Train: Loss = 0.01747, Accuracy = 99.44%: 100%|██████████| 572/572 [00:07<00:00, 77.14it/s]\n",
            "[6 / 50]   Val: Loss = 0.09683, Accuracy = 97.33%: 100%|██████████| 13/13 [00:00<00:00, 46.03it/s]\n",
            "[7 / 50] Train: Loss = 0.01330, Accuracy = 99.58%: 100%|██████████| 572/572 [00:07<00:00, 76.60it/s]\n",
            "[7 / 50]   Val: Loss = 0.09782, Accuracy = 97.44%: 100%|██████████| 13/13 [00:00<00:00, 47.83it/s]\n",
            "[8 / 50] Train: Loss = 0.00976, Accuracy = 99.70%: 100%|██████████| 572/572 [00:07<00:00, 76.62it/s]\n",
            "[8 / 50]   Val: Loss = 0.10219, Accuracy = 97.35%: 100%|██████████| 13/13 [00:00<00:00, 48.37it/s]\n",
            "[9 / 50] Train: Loss = 0.00697, Accuracy = 99.80%: 100%|██████████| 572/572 [00:07<00:00, 76.16it/s]\n",
            "[9 / 50]   Val: Loss = 0.11473, Accuracy = 97.17%: 100%|██████████| 13/13 [00:00<00:00, 47.00it/s]\n",
            "[10 / 50] Train: Loss = 0.00492, Accuracy = 99.86%: 100%|██████████| 572/572 [00:07<00:00, 76.45it/s]\n",
            "[10 / 50]   Val: Loss = 0.11937, Accuracy = 97.25%: 100%|██████████| 13/13 [00:00<00:00, 47.91it/s]\n",
            "[11 / 50] Train: Loss = 0.00325, Accuracy = 99.92%: 100%|██████████| 572/572 [00:07<00:00, 76.67it/s]\n",
            "[11 / 50]   Val: Loss = 0.12997, Accuracy = 97.05%: 100%|██████████| 13/13 [00:00<00:00, 48.78it/s]\n",
            "[12 / 50] Train: Loss = 0.00228, Accuracy = 99.95%: 100%|██████████| 572/572 [00:07<00:00, 76.42it/s]\n",
            "[12 / 50]   Val: Loss = 0.12956, Accuracy = 97.21%: 100%|██████████| 13/13 [00:00<00:00, 48.40it/s]\n",
            "[13 / 50] Train: Loss = 0.00143, Accuracy = 99.97%: 100%|██████████| 572/572 [00:07<00:00, 76.41it/s]\n",
            "[13 / 50]   Val: Loss = 0.14321, Accuracy = 97.16%: 100%|██████████| 13/13 [00:00<00:00, 47.78it/s]\n",
            "[14 / 50] Train: Loss = 0.00124, Accuracy = 99.98%: 100%|██████████| 572/572 [00:07<00:00, 76.23it/s]\n",
            "[14 / 50]   Val: Loss = 0.14765, Accuracy = 97.11%: 100%|██████████| 13/13 [00:00<00:00, 47.19it/s]\n",
            "[15 / 50] Train: Loss = 0.00106, Accuracy = 99.98%: 100%|██████████| 572/572 [00:07<00:00, 76.50it/s]\n",
            "[15 / 50]   Val: Loss = 0.15610, Accuracy = 97.07%: 100%|██████████| 13/13 [00:00<00:00, 47.53it/s]\n",
            "[16 / 50] Train: Loss = 0.00079, Accuracy = 99.98%: 100%|██████████| 572/572 [00:07<00:00, 75.37it/s]\n",
            "[16 / 50]   Val: Loss = 0.16823, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 47.73it/s]\n",
            "[17 / 50] Train: Loss = 0.00124, Accuracy = 99.97%: 100%|██████████| 572/572 [00:07<00:00, 75.51it/s]\n",
            "[17 / 50]   Val: Loss = 0.17515, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 49.62it/s]\n",
            "[18 / 50] Train: Loss = 0.00196, Accuracy = 99.94%: 100%|██████████| 572/572 [00:07<00:00, 75.78it/s]\n",
            "[18 / 50]   Val: Loss = 0.16984, Accuracy = 96.98%: 100%|██████████| 13/13 [00:00<00:00, 47.68it/s]\n",
            "[19 / 50] Train: Loss = 0.00055, Accuracy = 99.99%: 100%|██████████| 572/572 [00:07<00:00, 74.90it/s]\n",
            "[19 / 50]   Val: Loss = 0.17201, Accuracy = 97.00%: 100%|██████████| 13/13 [00:00<00:00, 46.62it/s]\n",
            "[20 / 50] Train: Loss = 0.00020, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 75.66it/s]\n",
            "[20 / 50]   Val: Loss = 0.17683, Accuracy = 97.04%: 100%|██████████| 13/13 [00:00<00:00, 46.21it/s]\n",
            "[21 / 50] Train: Loss = 0.00012, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 74.13it/s]\n",
            "[21 / 50]   Val: Loss = 0.18046, Accuracy = 97.05%: 100%|██████████| 13/13 [00:00<00:00, 45.62it/s]\n",
            "[22 / 50] Train: Loss = 0.00010, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 74.74it/s]\n",
            "[22 / 50]   Val: Loss = 0.17772, Accuracy = 97.11%: 100%|██████████| 13/13 [00:00<00:00, 44.48it/s]\n",
            "[23 / 50] Train: Loss = 0.00008, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 74.38it/s]\n",
            "[23 / 50]   Val: Loss = 0.18212, Accuracy = 97.06%: 100%|██████████| 13/13 [00:00<00:00, 50.62it/s]\n",
            "[24 / 50] Train: Loss = 0.00012, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 74.02it/s]\n",
            "[24 / 50]   Val: Loss = 0.19376, Accuracy = 96.87%: 100%|██████████| 13/13 [00:00<00:00, 46.07it/s]\n",
            "[25 / 50] Train: Loss = 0.00375, Accuracy = 99.88%: 100%|██████████| 572/572 [00:07<00:00, 74.75it/s]\n",
            "[25 / 50]   Val: Loss = 0.18172, Accuracy = 96.94%: 100%|██████████| 13/13 [00:00<00:00, 49.18it/s]\n",
            "[26 / 50] Train: Loss = 0.00062, Accuracy = 99.98%: 100%|██████████| 572/572 [00:07<00:00, 74.10it/s]\n",
            "[26 / 50]   Val: Loss = 0.18958, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 47.11it/s]\n",
            "[27 / 50] Train: Loss = 0.00018, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 74.83it/s]\n",
            "[27 / 50]   Val: Loss = 0.19066, Accuracy = 96.96%: 100%|██████████| 13/13 [00:00<00:00, 48.01it/s]\n",
            "[28 / 50] Train: Loss = 0.00007, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 74.78it/s]\n",
            "[28 / 50]   Val: Loss = 0.18920, Accuracy = 97.01%: 100%|██████████| 13/13 [00:00<00:00, 47.07it/s]\n",
            "[29 / 50] Train: Loss = 0.00006, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 73.72it/s]\n",
            "[29 / 50]   Val: Loss = 0.19139, Accuracy = 97.01%: 100%|██████████| 13/13 [00:00<00:00, 46.80it/s]\n",
            "[30 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 73.94it/s]\n",
            "[30 / 50]   Val: Loss = 0.19152, Accuracy = 97.04%: 100%|██████████| 13/13 [00:00<00:00, 47.66it/s]\n",
            "[31 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 74.31it/s]\n",
            "[31 / 50]   Val: Loss = 0.19930, Accuracy = 96.98%: 100%|██████████| 13/13 [00:00<00:00, 46.23it/s]\n",
            "[32 / 50] Train: Loss = 0.00235, Accuracy = 99.92%: 100%|██████████| 572/572 [00:07<00:00, 73.80it/s]\n",
            "[32 / 50]   Val: Loss = 0.19444, Accuracy = 97.00%: 100%|██████████| 13/13 [00:00<00:00, 47.24it/s]\n",
            "[33 / 50] Train: Loss = 0.00088, Accuracy = 99.97%: 100%|██████████| 572/572 [00:07<00:00, 73.60it/s]\n",
            "[33 / 50]   Val: Loss = 0.19868, Accuracy = 96.96%: 100%|██████████| 13/13 [00:00<00:00, 46.82it/s]\n",
            "[34 / 50] Train: Loss = 0.00025, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 73.40it/s]\n",
            "[34 / 50]   Val: Loss = 0.20254, Accuracy = 96.98%: 100%|██████████| 13/13 [00:00<00:00, 46.72it/s]\n",
            "[35 / 50] Train: Loss = 0.00007, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 73.64it/s]\n",
            "[35 / 50]   Val: Loss = 0.20087, Accuracy = 97.04%: 100%|██████████| 13/13 [00:00<00:00, 49.56it/s]\n",
            "[36 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 73.14it/s]\n",
            "[36 / 50]   Val: Loss = 0.20297, Accuracy = 97.04%: 100%|██████████| 13/13 [00:00<00:00, 45.82it/s]\n",
            "[37 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 73.06it/s]\n",
            "[37 / 50]   Val: Loss = 0.20300, Accuracy = 97.07%: 100%|██████████| 13/13 [00:00<00:00, 46.01it/s]\n",
            "[38 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 72.89it/s]\n",
            "[38 / 50]   Val: Loss = 0.20869, Accuracy = 97.01%: 100%|██████████| 13/13 [00:00<00:00, 51.75it/s]\n",
            "[39 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 73.03it/s]\n",
            "[39 / 50]   Val: Loss = 0.20653, Accuracy = 97.08%: 100%|██████████| 13/13 [00:00<00:00, 48.49it/s]\n",
            "[40 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 73.00it/s]\n",
            "[40 / 50]   Val: Loss = 0.20705, Accuracy = 97.07%: 100%|██████████| 13/13 [00:00<00:00, 47.79it/s]\n",
            "[41 / 50] Train: Loss = 0.00280, Accuracy = 99.91%: 100%|██████████| 572/572 [00:07<00:00, 72.71it/s]\n",
            "[41 / 50]   Val: Loss = 0.20353, Accuracy = 96.87%: 100%|██████████| 13/13 [00:00<00:00, 47.74it/s]\n",
            "[42 / 50] Train: Loss = 0.00068, Accuracy = 99.98%: 100%|██████████| 572/572 [00:07<00:00, 72.16it/s]\n",
            "[42 / 50]   Val: Loss = 0.21000, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 45.62it/s]\n",
            "[43 / 50] Train: Loss = 0.00012, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 72.19it/s]\n",
            "[43 / 50]   Val: Loss = 0.20520, Accuracy = 97.01%: 100%|██████████| 13/13 [00:00<00:00, 45.65it/s]\n",
            "[44 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 72.32it/s]\n",
            "[44 / 50]   Val: Loss = 0.20912, Accuracy = 96.98%: 100%|██████████| 13/13 [00:00<00:00, 49.62it/s]\n",
            "[45 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 72.07it/s]\n",
            "[45 / 50]   Val: Loss = 0.20921, Accuracy = 96.99%: 100%|██████████| 13/13 [00:00<00:00, 46.28it/s]\n",
            "[46 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 72.09it/s]\n",
            "[46 / 50]   Val: Loss = 0.21245, Accuracy = 97.01%: 100%|██████████| 13/13 [00:00<00:00, 46.48it/s]\n",
            "[47 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 72.22it/s]\n",
            "[47 / 50]   Val: Loss = 0.20990, Accuracy = 96.99%: 100%|██████████| 13/13 [00:00<00:00, 48.30it/s]\n",
            "[48 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:08<00:00, 71.30it/s]\n",
            "[48 / 50]   Val: Loss = 0.21197, Accuracy = 97.01%: 100%|██████████| 13/13 [00:00<00:00, 48.38it/s]\n",
            "[49 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 71.95it/s]\n",
            "[49 / 50]   Val: Loss = 0.21690, Accuracy = 96.95%: 100%|██████████| 13/13 [00:00<00:00, 46.39it/s]\n",
            "[50 / 50] Train: Loss = 0.00007, Accuracy = 100.00%: 100%|██████████| 572/572 [00:08<00:00, 71.49it/s]\n",
            "[50 / 50]   Val: Loss = 0.22347, Accuracy = 96.83%: 100%|██████████| 13/13 [00:00<00:00, 46.98it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPUuAPGhEGVR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c8a5e9-9a35-4375-f7b9-efb6928e8d61"
      },
      "source": [
        "loss, accuracy = do_epoch(model, criterion, (X_test, y_test), 512, optimizer=None, name='test')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " test Loss = 0.22437, Accuracy = 96.88%: 100%|██████████| 28/28 [00:00<00:00, 50.60it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}