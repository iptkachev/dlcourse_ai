{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "\n",
    "def check_gradient(f, x, delta=1e-5, tol = 1e-4):\n",
    "    '''\n",
    "    Checks the implementation of analytical gradient by comparing\n",
    "    it to numerical gradient using two-point formula\n",
    "\n",
    "    Arguments:\n",
    "      f: function that receives x and computes value and gradient\n",
    "      x: np array, initial point where gradient is checked\n",
    "      delta: step to compute numerical gradient\n",
    "      tol: tolerance for comparing numerical and analytical gradient\n",
    "\n",
    "    Return:\n",
    "      bool indicating whether gradients match or not\n",
    "    '''\n",
    "    \n",
    "    assert isinstance(x, np.ndarray)\n",
    "    assert x.dtype == np.float\n",
    "    \n",
    "    orig_x = x.copy()\n",
    "    fx, analytic_grad = f(x)\n",
    "    assert np.all(np.isclose(orig_x, x, tol)), \"Functions shouldn't modify input variables\"\n",
    "\n",
    "    assert analytic_grad.shape == x.shape\n",
    "    analytic_grad = analytic_grad.copy()\n",
    "\n",
    "    # We will go through every dimension of x and compute numeric\n",
    "    # derivative for it\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        analytic_grad_at_ix = analytic_grad[ix]\n",
    "        \n",
    "        x_plus_delta = x.copy()\n",
    "        x_minus_delta = x.copy()\n",
    "        x_plus_delta[ix] = x_plus_delta[ix] + delta\n",
    "        x_minus_delta[ix] = x_minus_delta[ix] - delta\n",
    "        numeric_grad_at_ix = (f(x_plus_delta)[0] - f(x_minus_delta)[0]) / (2 * delta)\n",
    "        \n",
    "        # TODO compute value of numeric gradient of f to idx\n",
    "        if not np.isclose(numeric_grad_at_ix, analytic_grad_at_ix, tol):\n",
    "            print(\"Gradients are different at %s. Analytic: %2.5f, Numeric: %2.5f\" % (ix, analytic_grad_at_ix, numeric_grad_at_ix))\n",
    "            return False\n",
    "        \n",
    "        it.iternext()\n",
    "\n",
    "    print(\"Gradient check passed!\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(5)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(predictions):\n",
    "    '''\n",
    "    Computes probabilities from scores\n",
    "\n",
    "    Arguments:\n",
    "      predictions, np array, shape is either (N) or (batch_size, N) -\n",
    "        classifier output\n",
    "\n",
    "    Returns:\n",
    "      probs, np array of the same shape as predictions - \n",
    "        probability for every class, 0..1\n",
    "    '''\n",
    "    # TODO implement softmax\n",
    "    # Your final implementation shouldn't have any loops\n",
    "    predictions = np.copy(predictions)\n",
    "    \n",
    "    kwargs = dict()\n",
    "    if len(predictions.shape) == 2:\n",
    "        predictions -= np.max(predictions, axis=1).reshape(-1, 1)\n",
    "        exps = np.exp(predictions)\n",
    "        probs = exps / exps.sum(axis=1).reshape(-1, 1)\n",
    "    else:\n",
    "        predictions -= np.max(predictions)\n",
    "        exps = np.exp(predictions)\n",
    "        probs = exps / exps.sum()\n",
    "        \n",
    "    return probs\n",
    "\n",
    "\n",
    "def cross_entropy_loss(probs, target_index):\n",
    "    '''\n",
    "    Computes cross-entropy loss\n",
    "\n",
    "    Arguments:\n",
    "      probs, np array, shape is either (N) or (batch_size, N) -\n",
    "        probabilities for every class\n",
    "      target_index: np array of int, shape is (1) or (batch_size) -\n",
    "        index of the true class for given sample(s)\n",
    "\n",
    "    Returns:\n",
    "      loss: single value\n",
    "    '''\n",
    "    # TODO implement cross-entropy\n",
    "    # Your final implementation shouldn't have any loops\n",
    "    if len(probs.shape) == 2:\n",
    "        n_classes = probs.shape[1]\n",
    "        one_hot_true_classes = np.eye(n_classes)[np.array(target_index).reshape(-1)]\n",
    "    else:\n",
    "        n_classes = probs.shape[0]\n",
    "        one_hot_true_classes = np.eye(n_classes)[target_index]\n",
    "\n",
    "    \n",
    "    loss = -(one_hot_true_classes * np.log(probs)).sum()\n",
    "        \n",
    "    return loss\n",
    "\n",
    "\n",
    "def softmax_with_cross_entropy(predictions, target_index):\n",
    "    '''\n",
    "    Computes softmax and cross-entropy loss for model predictions,\n",
    "    including the gradient\n",
    "\n",
    "    Arguments:\n",
    "      predictions, np array, shape is either (N) or (batch_size, N) -\n",
    "        classifier output\n",
    "      target_index: np array of int, shape is (1) or (batch_size) -\n",
    "        index of the true class for given sample(s)\n",
    "\n",
    "    Returns:\n",
    "      loss, single value - cross-entropy loss\n",
    "      dprediction, np array same shape as predictions - gradient of predictions by loss value\n",
    "    '''\n",
    "    # TODO implement softmax with cross-entropy\n",
    "    # Your final implementation shouldn't have any loops\n",
    "    probs = softmax(predictions)\n",
    "    loss = cross_entropy_loss(probs, target_index)\n",
    "    \n",
    "    if len(probs.shape) == 2:\n",
    "        n_classes = probs.shape[1]\n",
    "        one_hot_true_classes = np.eye(n_classes)[np.array(target_index).reshape(-1)]\n",
    "    else:\n",
    "        n_classes = probs.shape[0]\n",
    "        one_hot_true_classes = np.eye(n_classes)[target_index]\n",
    "\n",
    "    dprediction = probs - one_hot_true_classes  # https://www.youtube.com/watch?v=bZihskzsSjM&t=1090s\n",
    "    \n",
    "    return loss, dprediction\n",
    "\n",
    "\n",
    "def l2_regularization(W, reg_strength):\n",
    "    '''\n",
    "    Computes L2 regularization loss on weights and its gradient\n",
    "\n",
    "    Arguments:\n",
    "      W, np array - weights\n",
    "      reg_strength - float value\n",
    "\n",
    "    Returns:\n",
    "      loss, single value - l2 regularization loss\n",
    "      gradient, np.array same shape as W - gradient of weight by l2 loss\n",
    "    '''\n",
    "\n",
    "    # TODO: implement l2 regularization and gradient\n",
    "    # Your final implementation shouldn't have any loops\n",
    "    loss = reg_strength * np.power(W, 2).sum()\n",
    "    grad = 2 * reg_strength * W\n",
    "    return loss, grad\n",
    "    \n",
    "\n",
    "def linear_softmax(X, W, target_index):\n",
    "    '''\n",
    "    Performs linear classification and returns loss and gradient over W\n",
    "\n",
    "    Arguments:\n",
    "      X, np array, shape (num_batch, num_features) - batch of images\n",
    "      W, np array, shape (num_features, classes) - weights\n",
    "      target_index, np array, shape (num_batch) - index of target classes\n",
    "\n",
    "    Returns:\n",
    "      loss, single value - cross-entropy loss\n",
    "      gradient, np.array same shape as W - gradient of weight by loss\n",
    "\n",
    "    '''\n",
    "    predictions = np.dot(X, W)\n",
    "    loss, dprediction = softmax_with_cross_entropy(predictions, target_index)\n",
    "    dW = X.T @ dprediction\n",
    "    # TODO implement prediction and gradient over W\n",
    "    # Your final implementation shouldn't have any loops\n",
    "    \n",
    "    return loss, dW\n",
    "\n",
    "\n",
    "class LinearSoftmaxClassifier():\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def fit(self, X, y, batch_size=100, learning_rate=1e-7, reg=1e-5,\n",
    "            epochs=1):\n",
    "        '''\n",
    "        Trains linear classifier\n",
    "        \n",
    "        Arguments:\n",
    "          X, np array (num_samples, num_features) - training data\n",
    "          y, np array of int (num_samples) - labels\n",
    "          batch_size, int - batch size to use\n",
    "          learning_rate, float - learning rate for gradient descent\n",
    "          reg, float - L2 regularization strength\n",
    "          epochs, int - number of epochs\n",
    "        '''\n",
    "\n",
    "        num_train = X.shape[0]\n",
    "        num_features = X.shape[1]\n",
    "        num_classes = np.max(y)+1\n",
    "        if self.W is None:\n",
    "            self.W = 0.001 * np.random.randn(num_features, num_classes)\n",
    "\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            shuffled_indices = np.arange(num_train)\n",
    "            np.random.shuffle(shuffled_indices)\n",
    "            sections = np.arange(batch_size, num_train, batch_size)\n",
    "            batches_indices = np.array_split(shuffled_indices, sections)\n",
    "\n",
    "            # TODO implement generating batches from indices\n",
    "            # Compute loss and gradients\n",
    "            # Apply gradient to weights using learning rate\n",
    "            # Don't forget to add both cross-entropy loss\n",
    "            # and regularization!\n",
    "            loss = 0.\n",
    "            for batch in batches_indices:\n",
    "                loss_W, grad_W = linear_softmax(X[batch], self.W, y[batch])\n",
    "                loss_W_l2, grad_W_l2 = l2_regularization(self.W, reg)\n",
    "                self.W -= learning_rate * (grad_W + grad_W_l2)\n",
    "                loss += loss_W_l2\n",
    "                loss += loss_W\n",
    "            loss_history.append(loss)\n",
    "            # end\n",
    "            print(\"Epoch %i, loss: %f\" % (epoch, loss))\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Produces classifier predictions on the set\n",
    "       \n",
    "        Arguments:\n",
    "          X, np array (test_samples, num_features)\n",
    "\n",
    "        Returns:\n",
    "          y_pred, np.array of int (test_samples)\n",
    "        '''\n",
    "        y_pred = np.zeros(X.shape[0], dtype=np.int)\n",
    "\n",
    "        # TODO Implement class prediction\n",
    "        # Your final implementation shouldn't have any loops\n",
    "        predictions = np.dot(X, self.W)\n",
    "        y_pred = np.argmax(softmax(predictions), axis=1)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = softmax(np.array([-5, 0, 5]))\n",
    "cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 20653.061912\n",
      "Epoch 1, loss: 20487.736576\n",
      "Epoch 2, loss: 20354.186673\n",
      "Epoch 3, loss: 20259.398840\n",
      "Epoch 4, loss: 20184.480043\n",
      "Epoch 5, loss: 20141.812324\n",
      "Epoch 6, loss: 20097.280886\n",
      "Epoch 7, loss: 20061.933577\n",
      "Epoch 8, loss: 20042.227651\n",
      "Epoch 9, loss: 20018.380936\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-4, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11c1573a0>]"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.23\n",
      "Epoch 0, loss: 19985.394438\n",
      "Epoch 1, loss: 19979.843614\n",
      "Epoch 2, loss: 19976.813079\n",
      "Epoch 3, loss: 19975.154714\n",
      "Epoch 4, loss: 19973.195170\n",
      "Epoch 5, loss: 19972.567599\n",
      "Epoch 6, loss: 19971.097281\n",
      "Epoch 7, loss: 19968.995500\n",
      "Epoch 8, loss: 19969.295326\n",
      "Epoch 9, loss: 19967.112184\n",
      "Epoch 10, loss: 19965.688164\n",
      "Epoch 11, loss: 19964.706682\n",
      "Epoch 12, loss: 19963.832044\n",
      "Epoch 13, loss: 19963.056186\n",
      "Epoch 14, loss: 19961.611321\n",
      "Epoch 15, loss: 19960.718776\n",
      "Epoch 16, loss: 19959.583852\n",
      "Epoch 17, loss: 19958.564644\n",
      "Epoch 18, loss: 19957.841296\n",
      "Epoch 19, loss: 19956.484197\n",
      "Epoch 20, loss: 19956.506042\n",
      "Epoch 21, loss: 19954.896441\n",
      "Epoch 22, loss: 19954.404021\n",
      "Epoch 23, loss: 19953.218928\n",
      "Epoch 24, loss: 19951.995318\n",
      "Epoch 25, loss: 19951.940168\n",
      "Epoch 26, loss: 19950.568670\n",
      "Epoch 27, loss: 19949.739932\n",
      "Epoch 28, loss: 19949.349159\n",
      "Epoch 29, loss: 19948.649033\n",
      "Epoch 30, loss: 19947.840497\n",
      "Epoch 31, loss: 19947.404098\n",
      "Epoch 32, loss: 19946.033280\n",
      "Epoch 33, loss: 19945.243037\n",
      "Epoch 34, loss: 19945.049214\n",
      "Epoch 35, loss: 19944.792978\n",
      "Epoch 36, loss: 19944.213311\n",
      "Epoch 37, loss: 19943.167275\n",
      "Epoch 38, loss: 19942.139069\n",
      "Epoch 39, loss: 19941.892528\n",
      "Epoch 40, loss: 19941.942354\n",
      "Epoch 41, loss: 19940.872313\n",
      "Epoch 42, loss: 19940.710679\n",
      "Epoch 43, loss: 19939.789857\n",
      "Epoch 44, loss: 19938.747317\n",
      "Epoch 45, loss: 19938.683324\n",
      "Epoch 46, loss: 19937.326389\n",
      "Epoch 47, loss: 19937.169030\n",
      "Epoch 48, loss: 19936.986146\n",
      "Epoch 49, loss: 19936.342510\n",
      "Epoch 50, loss: 19935.725033\n",
      "Epoch 51, loss: 19935.714303\n",
      "Epoch 52, loss: 19934.575405\n",
      "Epoch 53, loss: 19934.704108\n",
      "Epoch 54, loss: 19934.650932\n",
      "Epoch 55, loss: 19933.032105\n",
      "Epoch 56, loss: 19933.391357\n",
      "Epoch 57, loss: 19932.781785\n",
      "Epoch 58, loss: 19932.598966\n",
      "Epoch 59, loss: 19931.970539\n",
      "Epoch 60, loss: 19931.320453\n",
      "Epoch 61, loss: 19931.343427\n",
      "Epoch 62, loss: 19931.380926\n",
      "Epoch 63, loss: 19930.487352\n",
      "Epoch 64, loss: 19930.668646\n",
      "Epoch 65, loss: 19929.632673\n",
      "Epoch 66, loss: 19929.543622\n",
      "Epoch 67, loss: 19929.170617\n",
      "Epoch 68, loss: 19928.881618\n",
      "Epoch 69, loss: 19928.429158\n",
      "Epoch 70, loss: 19928.171521\n",
      "Epoch 71, loss: 19927.455513\n",
      "Epoch 72, loss: 19927.433384\n",
      "Epoch 73, loss: 19927.050287\n",
      "Epoch 74, loss: 19926.634901\n",
      "Epoch 75, loss: 19926.707815\n",
      "Epoch 76, loss: 19926.375166\n",
      "Epoch 77, loss: 19925.915909\n",
      "Epoch 78, loss: 19925.574249\n",
      "Epoch 79, loss: 19925.654960\n",
      "Epoch 80, loss: 19925.274737\n",
      "Epoch 81, loss: 19924.958581\n",
      "Epoch 82, loss: 19924.579793\n",
      "Epoch 83, loss: 19924.080652\n",
      "Epoch 84, loss: 19923.849982\n",
      "Epoch 85, loss: 19923.960840\n",
      "Epoch 86, loss: 19923.421832\n",
      "Epoch 87, loss: 19923.536170\n",
      "Epoch 88, loss: 19922.990034\n",
      "Epoch 89, loss: 19923.443184\n",
      "Epoch 90, loss: 19922.594664\n",
      "Epoch 91, loss: 19922.743074\n",
      "Epoch 92, loss: 19922.541207\n",
      "Epoch 93, loss: 19921.977630\n",
      "Epoch 94, loss: 19922.066809\n",
      "Epoch 95, loss: 19921.647954\n",
      "Epoch 96, loss: 19921.407388\n",
      "Epoch 97, loss: 19921.324329\n",
      "Epoch 98, loss: 19920.848893\n",
      "Epoch 99, loss: 19921.185219\n",
      "Accuracy after training for 100 epochs:  0.229\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-5, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.0001\n",
      "Epoch 0, loss: 22726.463864\n",
      "Epoch 1, loss: 23779.930313\n",
      "Epoch 2, loss: 21666.675840\n",
      "Epoch 3, loss: 24656.184198\n",
      "Epoch 4, loss: 22640.439003\n",
      "Epoch 5, loss: 22503.814588\n",
      "Epoch 6, loss: 21472.008426\n",
      "Epoch 7, loss: 22548.327715\n",
      "Epoch 8, loss: 23171.539922\n",
      "Epoch 9, loss: 23247.644472\n",
      "Epoch 10, loss: 21975.956773\n",
      "Epoch 11, loss: 23826.704886\n",
      "Epoch 12, loss: 23210.519961\n",
      "Epoch 13, loss: 23362.861635\n",
      "Epoch 14, loss: 21586.384343\n",
      "Epoch 15, loss: 22739.066010\n",
      "Epoch 16, loss: 24019.131082\n",
      "Epoch 17, loss: 21035.369668\n",
      "Epoch 18, loss: 23964.669096\n",
      "Epoch 19, loss: 22744.617266\n",
      "Epoch 20, loss: 23881.528421\n",
      "Epoch 21, loss: 22375.206380\n",
      "Epoch 22, loss: 22556.745665\n",
      "Epoch 23, loss: 23252.103705\n",
      "Epoch 24, loss: 22153.984070\n",
      "Epoch 25, loss: 22780.590840\n",
      "Epoch 26, loss: 23858.149114\n",
      "Epoch 27, loss: 22991.980989\n",
      "Epoch 28, loss: 23384.710533\n",
      "Epoch 29, loss: 23265.613822\n",
      "Epoch 30, loss: 22511.938624\n",
      "Epoch 31, loss: 22701.332944\n",
      "Epoch 32, loss: 23765.581547\n",
      "Epoch 33, loss: 22546.671687\n",
      "Epoch 34, loss: 21615.535760\n",
      "Epoch 35, loss: 22307.932744\n",
      "Epoch 36, loss: 21649.967566\n",
      "Epoch 37, loss: 23920.306123\n",
      "Epoch 38, loss: 22764.454092\n",
      "Epoch 39, loss: 22721.002457\n",
      "Epoch 40, loss: 23072.225674\n",
      "Epoch 41, loss: 22198.244364\n",
      "Epoch 42, loss: 22259.225818\n",
      "Epoch 43, loss: 23310.253773\n",
      "Epoch 44, loss: 22534.779813\n",
      "Epoch 45, loss: 23269.989954\n",
      "Epoch 46, loss: 22297.529675\n",
      "Epoch 47, loss: 23591.759873\n",
      "Epoch 48, loss: 23127.373488\n",
      "Epoch 49, loss: 22483.801840\n",
      "Epoch 50, loss: 23149.687175\n",
      "Epoch 51, loss: 23046.795515\n",
      "Epoch 52, loss: 23838.648803\n",
      "Epoch 53, loss: 21927.122858\n",
      "Epoch 54, loss: 19817.917780\n",
      "Epoch 55, loss: 22758.846500\n",
      "Epoch 56, loss: 22871.810227\n",
      "Epoch 57, loss: 22846.315020\n",
      "Epoch 58, loss: 22027.675000\n",
      "Epoch 59, loss: 24048.624167\n",
      "Epoch 60, loss: 22039.182194\n",
      "Epoch 61, loss: 21508.961125\n",
      "Epoch 62, loss: 23111.761641\n",
      "Epoch 63, loss: 21611.441055\n",
      "Epoch 64, loss: 22746.076728\n",
      "Epoch 65, loss: 22485.738054\n",
      "Epoch 66, loss: 23407.297034\n",
      "Epoch 67, loss: 22617.499487\n",
      "Epoch 68, loss: 22334.548308\n",
      "Epoch 69, loss: 21897.528628\n",
      "Epoch 70, loss: 21676.678830\n",
      "Epoch 71, loss: 23154.220034\n",
      "Epoch 72, loss: 21575.413471\n",
      "Epoch 73, loss: 21372.566696\n",
      "Epoch 74, loss: 21723.624687\n",
      "Epoch 75, loss: 23722.323668\n",
      "Epoch 76, loss: 22462.749015\n",
      "Epoch 77, loss: 22415.723404\n",
      "Epoch 78, loss: 21968.338481\n",
      "Epoch 79, loss: 22455.904283\n",
      "Epoch 80, loss: 22861.938354\n",
      "Epoch 81, loss: 22422.596813\n",
      "Epoch 82, loss: 21923.994857\n",
      "Epoch 83, loss: 22504.303008\n",
      "Epoch 84, loss: 22364.400652\n",
      "Epoch 85, loss: 22709.875099\n",
      "Epoch 86, loss: 22213.930683\n",
      "Epoch 87, loss: 23677.446702\n",
      "Epoch 88, loss: 21643.736304\n",
      "Epoch 89, loss: 22976.376989\n",
      "Epoch 90, loss: 22129.866279\n",
      "Epoch 91, loss: 22473.188183\n",
      "Epoch 92, loss: 22995.400870\n",
      "Epoch 93, loss: 21296.503587\n",
      "Epoch 94, loss: 22110.727895\n",
      "Epoch 95, loss: 22163.194255\n",
      "Epoch 96, loss: 22908.603625\n",
      "Epoch 97, loss: 22017.463196\n",
      "Epoch 98, loss: 21122.672021\n",
      "Epoch 99, loss: 22078.240318\n",
      "New best classifier with accuracy=0.194\n",
      "\n",
      "0.001 1e-05\n",
      "Epoch 0, loss: 20938.017159\n",
      "Epoch 1, loss: 22427.409775\n",
      "Epoch 2, loss: 23435.235468\n",
      "Epoch 3, loss: 21983.313012\n",
      "Epoch 4, loss: 21687.747425\n",
      "Epoch 5, loss: 22928.078057\n",
      "Epoch 6, loss: 20803.746335\n",
      "Epoch 7, loss: 22623.631506\n",
      "Epoch 8, loss: 22894.096355\n",
      "Epoch 9, loss: 22008.111890\n",
      "Epoch 10, loss: 22702.894064\n",
      "Epoch 11, loss: 20915.656476\n",
      "Epoch 12, loss: 22109.774398\n",
      "Epoch 13, loss: 22018.743568\n",
      "Epoch 14, loss: 23116.630045\n",
      "Epoch 15, loss: 22405.433465\n",
      "Epoch 16, loss: 22828.560044\n",
      "Epoch 17, loss: 21393.400827\n",
      "Epoch 18, loss: 21597.847832\n",
      "Epoch 19, loss: 22126.050928\n",
      "Epoch 20, loss: 22560.572102\n",
      "Epoch 21, loss: 22146.134048\n",
      "Epoch 22, loss: 22002.306889\n",
      "Epoch 23, loss: 22646.695692\n",
      "Epoch 24, loss: 22994.924090\n",
      "Epoch 25, loss: 22174.048133\n",
      "Epoch 26, loss: 21493.222173\n",
      "Epoch 27, loss: 22711.283202\n",
      "Epoch 28, loss: 22353.895529\n",
      "Epoch 29, loss: 23072.041811\n",
      "Epoch 30, loss: 21648.378100\n",
      "Epoch 31, loss: 23240.605242\n",
      "Epoch 32, loss: 21222.857672\n",
      "Epoch 33, loss: 22138.063511\n",
      "Epoch 34, loss: 21689.694213\n",
      "Epoch 35, loss: 22472.812538\n",
      "Epoch 36, loss: 21374.836078\n",
      "Epoch 37, loss: 20398.313511\n",
      "Epoch 38, loss: 21620.697836\n",
      "Epoch 39, loss: 20602.722756\n",
      "Epoch 40, loss: 21994.100344\n",
      "Epoch 41, loss: 22599.656108\n",
      "Epoch 42, loss: 22029.228200\n",
      "Epoch 43, loss: 21915.252166\n",
      "Epoch 44, loss: 22567.522035\n",
      "Epoch 45, loss: 22129.564239\n",
      "Epoch 46, loss: 22815.815845\n",
      "Epoch 47, loss: 20974.224536\n",
      "Epoch 48, loss: 22454.237828\n",
      "Epoch 49, loss: 21561.565293\n",
      "Epoch 50, loss: 21629.544639\n",
      "Epoch 51, loss: 22187.425421\n",
      "Epoch 52, loss: 22214.794211\n",
      "Epoch 53, loss: 23635.949147\n",
      "Epoch 54, loss: 21419.767265\n",
      "Epoch 55, loss: 21009.991543\n",
      "Epoch 56, loss: 21931.290056\n",
      "Epoch 57, loss: 20049.424222\n",
      "Epoch 58, loss: 22705.709762\n",
      "Epoch 59, loss: 21846.191034\n",
      "Epoch 60, loss: 22995.266059\n",
      "Epoch 61, loss: 22650.420132\n",
      "Epoch 62, loss: 22669.753329\n",
      "Epoch 63, loss: 21291.126185\n",
      "Epoch 64, loss: 22208.634719\n",
      "Epoch 65, loss: 19743.212763\n",
      "Epoch 66, loss: 21569.413846\n",
      "Epoch 67, loss: 22253.343377\n",
      "Epoch 68, loss: 23053.603242\n",
      "Epoch 69, loss: 22189.184802\n",
      "Epoch 70, loss: 20838.727629\n",
      "Epoch 71, loss: 21825.162042\n",
      "Epoch 72, loss: 22530.288541\n",
      "Epoch 73, loss: 23466.182395\n",
      "Epoch 74, loss: 21871.352707\n",
      "Epoch 75, loss: 21836.554944\n",
      "Epoch 76, loss: 21520.488187\n",
      "Epoch 77, loss: 22158.320716\n",
      "Epoch 78, loss: 23823.517069\n",
      "Epoch 79, loss: 21563.226102\n",
      "Epoch 80, loss: 22109.386569\n",
      "Epoch 81, loss: 22087.416665\n",
      "Epoch 82, loss: 21232.854980\n",
      "Epoch 83, loss: 20952.789631\n",
      "Epoch 84, loss: 21376.298164\n",
      "Epoch 85, loss: 21859.601048\n",
      "Epoch 86, loss: 21001.781181\n",
      "Epoch 87, loss: 22020.985639\n",
      "Epoch 88, loss: 21458.581859\n",
      "Epoch 89, loss: 22381.363868\n",
      "Epoch 90, loss: 21141.891919\n",
      "Epoch 91, loss: 22595.955471\n",
      "Epoch 92, loss: 21004.468236\n",
      "Epoch 93, loss: 21639.983912\n",
      "Epoch 94, loss: 21966.116574\n",
      "Epoch 95, loss: 21296.830097\n",
      "Epoch 96, loss: 22741.688153\n",
      "Epoch 97, loss: 20941.017641\n",
      "Epoch 98, loss: 21939.060273\n",
      "Epoch 99, loss: 22680.232412\n",
      "New best classifier with accuracy=0.195\n",
      "\n",
      "0.001 1e-06\n",
      "Epoch 0, loss: 20534.751198\n",
      "Epoch 1, loss: 22096.239700\n",
      "Epoch 2, loss: 21602.685236\n",
      "Epoch 3, loss: 22187.859516\n",
      "Epoch 4, loss: 20955.632345\n",
      "Epoch 5, loss: 21112.105399\n",
      "Epoch 6, loss: 21698.797231\n",
      "Epoch 7, loss: 21817.457963\n",
      "Epoch 8, loss: 21047.325234\n",
      "Epoch 9, loss: 21140.628890\n",
      "Epoch 10, loss: 20476.102661\n",
      "Epoch 11, loss: 22087.243076\n",
      "Epoch 12, loss: 21873.332350\n",
      "Epoch 13, loss: 22899.490021\n",
      "Epoch 14, loss: 21802.926360\n",
      "Epoch 15, loss: 21305.861159\n",
      "Epoch 16, loss: 22075.251470\n",
      "Epoch 17, loss: 20546.518527\n",
      "Epoch 18, loss: 22202.262582\n",
      "Epoch 19, loss: 21545.429830\n",
      "Epoch 20, loss: 20701.792880\n",
      "Epoch 21, loss: 21773.059588\n",
      "Epoch 22, loss: 21370.556698\n",
      "Epoch 23, loss: 20721.928669\n",
      "Epoch 24, loss: 21967.474233\n",
      "Epoch 25, loss: 21845.101964\n",
      "Epoch 26, loss: 20778.881345\n",
      "Epoch 27, loss: 21511.070590\n",
      "Epoch 28, loss: 20767.740492\n",
      "Epoch 29, loss: 22469.134200\n",
      "Epoch 30, loss: 21960.491442\n",
      "Epoch 31, loss: 19571.173644\n",
      "Epoch 32, loss: 21826.252112\n",
      "Epoch 33, loss: 22292.434748\n",
      "Epoch 34, loss: 21466.087543\n",
      "Epoch 35, loss: 22395.861125\n",
      "Epoch 36, loss: 20771.705327\n",
      "Epoch 37, loss: 21046.016465\n",
      "Epoch 38, loss: 21438.290357\n",
      "Epoch 39, loss: 20847.316314\n",
      "Epoch 40, loss: 20916.271309\n",
      "Epoch 41, loss: 21515.911370\n",
      "Epoch 42, loss: 21604.191983\n",
      "Epoch 43, loss: 21697.466128\n",
      "Epoch 44, loss: 21743.495226\n",
      "Epoch 45, loss: 20906.355776\n",
      "Epoch 46, loss: 22290.148396\n",
      "Epoch 47, loss: 21552.652296\n",
      "Epoch 48, loss: 20424.610599\n",
      "Epoch 49, loss: 22428.207538\n",
      "Epoch 50, loss: 21125.404846\n",
      "Epoch 51, loss: 21534.027578\n",
      "Epoch 52, loss: 19717.896773\n",
      "Epoch 53, loss: 21613.334894\n",
      "Epoch 54, loss: 21619.487196\n",
      "Epoch 55, loss: 20888.112958\n",
      "Epoch 56, loss: 20911.908957\n",
      "Epoch 57, loss: 21753.832662\n",
      "Epoch 58, loss: 21107.802166\n",
      "Epoch 59, loss: 22857.374417\n",
      "Epoch 60, loss: 20741.297213\n",
      "Epoch 61, loss: 21072.477343\n",
      "Epoch 62, loss: 20738.925966\n",
      "Epoch 63, loss: 21467.735158\n",
      "Epoch 64, loss: 22742.985888\n",
      "Epoch 65, loss: 21640.612566\n",
      "Epoch 66, loss: 20384.691940\n",
      "Epoch 67, loss: 22300.451021\n",
      "Epoch 68, loss: 21779.123699\n",
      "Epoch 69, loss: 20650.860227\n",
      "Epoch 70, loss: 22339.503675\n",
      "Epoch 71, loss: 21154.522085\n",
      "Epoch 72, loss: 21161.338516\n",
      "Epoch 73, loss: 21792.861627\n",
      "Epoch 74, loss: 21945.364603\n",
      "Epoch 75, loss: 21217.965439\n",
      "Epoch 76, loss: 21624.894637\n",
      "Epoch 77, loss: 20175.227523\n",
      "Epoch 78, loss: 22634.511985\n",
      "Epoch 79, loss: 21110.811159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, loss: 20711.567457\n",
      "Epoch 81, loss: 21327.018104\n",
      "Epoch 82, loss: 22251.340404\n",
      "Epoch 83, loss: 21456.920736\n",
      "Epoch 84, loss: 20890.488010\n",
      "Epoch 85, loss: 20994.323592\n",
      "Epoch 86, loss: 20790.647647\n",
      "Epoch 87, loss: 21086.309116\n",
      "Epoch 88, loss: 20807.845617\n",
      "Epoch 89, loss: 21665.160268\n",
      "Epoch 90, loss: 21580.469778\n",
      "Epoch 91, loss: 22030.848587\n",
      "Epoch 92, loss: 21128.090782\n",
      "Epoch 93, loss: 20936.031369\n",
      "Epoch 94, loss: 22409.244242\n",
      "Epoch 95, loss: 20186.915988\n",
      "Epoch 96, loss: 21909.990367\n",
      "Epoch 97, loss: 20567.458250\n",
      "Epoch 98, loss: 20741.838832\n",
      "Epoch 99, loss: 22029.424343\n",
      "New best classifier with accuracy=0.201\n",
      "\n",
      "0.0001 0.0001\n",
      "Epoch 0, loss: 17678.117899\n",
      "Epoch 1, loss: 17241.062958\n",
      "Epoch 2, loss: 17182.776256\n",
      "Epoch 3, loss: 17138.782529\n",
      "Epoch 4, loss: 17101.788690\n",
      "Epoch 5, loss: 17071.326228\n",
      "Epoch 6, loss: 17043.729390\n",
      "Epoch 7, loss: 17019.897550\n",
      "Epoch 8, loss: 17007.390547\n",
      "Epoch 9, loss: 16993.494253\n",
      "Epoch 10, loss: 16973.617770\n",
      "Epoch 11, loss: 16972.836434\n",
      "Epoch 12, loss: 16964.581493\n",
      "Epoch 13, loss: 16954.648842\n",
      "Epoch 14, loss: 16945.609916\n",
      "Epoch 15, loss: 16932.964906\n",
      "Epoch 16, loss: 16930.608286\n",
      "Epoch 17, loss: 16927.725300\n",
      "Epoch 18, loss: 16923.420928\n",
      "Epoch 19, loss: 16915.316169\n",
      "Epoch 20, loss: 16921.904889\n",
      "Epoch 21, loss: 16916.656239\n",
      "Epoch 22, loss: 16909.378682\n",
      "Epoch 23, loss: 16905.309440\n",
      "Epoch 24, loss: 16910.197330\n",
      "Epoch 25, loss: 16905.611424\n",
      "Epoch 26, loss: 16903.864665\n",
      "Epoch 27, loss: 16901.589503\n",
      "Epoch 28, loss: 16899.396370\n",
      "Epoch 29, loss: 16903.167801\n",
      "Epoch 30, loss: 16897.781528\n",
      "Epoch 31, loss: 16892.019998\n",
      "Epoch 32, loss: 16894.564546\n",
      "Epoch 33, loss: 16893.196082\n",
      "Epoch 34, loss: 16889.421981\n",
      "Epoch 35, loss: 16894.314421\n",
      "Epoch 36, loss: 16887.543805\n",
      "Epoch 37, loss: 16889.441019\n",
      "Epoch 38, loss: 16889.981059\n",
      "Epoch 39, loss: 16885.932773\n",
      "Epoch 40, loss: 16887.422997\n",
      "Epoch 41, loss: 16885.436534\n",
      "Epoch 42, loss: 16882.162284\n",
      "Epoch 43, loss: 16888.574254\n",
      "Epoch 44, loss: 16881.289758\n",
      "Epoch 45, loss: 16882.768191\n",
      "Epoch 46, loss: 16876.940484\n",
      "Epoch 47, loss: 16878.191111\n",
      "Epoch 48, loss: 16877.457663\n",
      "Epoch 49, loss: 16881.586159\n",
      "Epoch 50, loss: 16879.952910\n",
      "Epoch 51, loss: 16874.787555\n",
      "Epoch 52, loss: 16879.415807\n",
      "Epoch 53, loss: 16881.625987\n",
      "Epoch 54, loss: 16871.615229\n",
      "Epoch 55, loss: 16874.860700\n",
      "Epoch 56, loss: 16874.852583\n",
      "Epoch 57, loss: 16868.850264\n",
      "Epoch 58, loss: 16874.674767\n",
      "Epoch 59, loss: 16874.755412\n",
      "Epoch 60, loss: 16868.627560\n",
      "Epoch 61, loss: 16871.778824\n",
      "Epoch 62, loss: 16869.858541\n",
      "Epoch 63, loss: 16869.275034\n",
      "Epoch 64, loss: 16870.450489\n",
      "Epoch 65, loss: 16873.995358\n",
      "Epoch 66, loss: 16867.024499\n",
      "Epoch 67, loss: 16869.553862\n",
      "Epoch 68, loss: 16866.785853\n",
      "Epoch 69, loss: 16863.167146\n",
      "Epoch 70, loss: 16861.943611\n",
      "Epoch 71, loss: 16865.032613\n",
      "Epoch 72, loss: 16866.143995\n",
      "Epoch 73, loss: 16863.174911\n",
      "Epoch 74, loss: 16862.643987\n",
      "Epoch 75, loss: 16865.805422\n",
      "Epoch 76, loss: 16862.738064\n",
      "Epoch 77, loss: 16859.082607\n",
      "Epoch 78, loss: 16863.580135\n",
      "Epoch 79, loss: 16860.231592\n",
      "Epoch 80, loss: 16866.424751\n",
      "Epoch 81, loss: 16857.607747\n",
      "Epoch 82, loss: 16857.419391\n",
      "Epoch 83, loss: 16863.192827\n",
      "Epoch 84, loss: 16857.995293\n",
      "Epoch 85, loss: 16861.087420\n",
      "Epoch 86, loss: 16860.545515\n",
      "Epoch 87, loss: 16856.757159\n",
      "Epoch 88, loss: 16854.722896\n",
      "Epoch 89, loss: 16857.145295\n",
      "Epoch 90, loss: 16854.092981\n",
      "Epoch 91, loss: 16856.762112\n",
      "Epoch 92, loss: 16857.707281\n",
      "Epoch 93, loss: 16854.419687\n",
      "Epoch 94, loss: 16853.534196\n",
      "Epoch 95, loss: 16854.482126\n",
      "Epoch 96, loss: 16856.089689\n",
      "Epoch 97, loss: 16857.621308\n",
      "Epoch 98, loss: 16849.731542\n",
      "Epoch 99, loss: 16846.684910\n",
      "New best classifier with accuracy=0.246\n",
      "\n",
      "0.0001 1e-05\n",
      "Epoch 0, loss: 16849.965344\n",
      "Epoch 1, loss: 16847.131347\n",
      "Epoch 2, loss: 16847.244804\n",
      "Epoch 3, loss: 16849.002018\n",
      "Epoch 4, loss: 16849.424599\n",
      "Epoch 5, loss: 16844.073379\n",
      "Epoch 6, loss: 16851.830065\n",
      "Epoch 7, loss: 16847.166782\n",
      "Epoch 8, loss: 16850.839976\n",
      "Epoch 9, loss: 16843.801055\n",
      "Epoch 10, loss: 16846.330976\n",
      "Epoch 11, loss: 16847.095485\n",
      "Epoch 12, loss: 16844.716013\n",
      "Epoch 13, loss: 16845.815882\n",
      "Epoch 14, loss: 16845.319524\n",
      "Epoch 15, loss: 16842.441898\n",
      "Epoch 16, loss: 16846.601589\n",
      "Epoch 17, loss: 16842.625875\n",
      "Epoch 18, loss: 16841.315224\n",
      "Epoch 19, loss: 16844.735053\n",
      "Epoch 20, loss: 16844.485233\n",
      "Epoch 21, loss: 16841.261467\n",
      "Epoch 22, loss: 16845.503054\n",
      "Epoch 23, loss: 16840.454136\n",
      "Epoch 24, loss: 16839.306716\n",
      "Epoch 25, loss: 16836.087794\n",
      "Epoch 26, loss: 16841.030726\n",
      "Epoch 27, loss: 16838.387463\n",
      "Epoch 28, loss: 16842.263805\n",
      "Epoch 29, loss: 16839.052536\n",
      "Epoch 30, loss: 16837.042081\n",
      "Epoch 31, loss: 16841.094312\n",
      "Epoch 32, loss: 16838.878299\n",
      "Epoch 33, loss: 16837.654773\n",
      "Epoch 34, loss: 16833.936429\n",
      "Epoch 35, loss: 16833.015485\n",
      "Epoch 36, loss: 16835.887659\n",
      "Epoch 37, loss: 16838.603791\n",
      "Epoch 38, loss: 16841.584519\n",
      "Epoch 39, loss: 16833.356082\n",
      "Epoch 40, loss: 16836.137488\n",
      "Epoch 41, loss: 16833.940538\n",
      "Epoch 42, loss: 16832.530752\n",
      "Epoch 43, loss: 16836.414005\n",
      "Epoch 44, loss: 16836.303921\n",
      "Epoch 45, loss: 16833.440759\n",
      "Epoch 46, loss: 16834.804957\n",
      "Epoch 47, loss: 16836.670327\n",
      "Epoch 48, loss: 16827.964838\n",
      "Epoch 49, loss: 16837.216142\n",
      "Epoch 50, loss: 16833.794435\n",
      "Epoch 51, loss: 16831.616176\n",
      "Epoch 52, loss: 16835.179729\n",
      "Epoch 53, loss: 16830.807098\n",
      "Epoch 54, loss: 16830.064699\n",
      "Epoch 55, loss: 16828.382379\n",
      "Epoch 56, loss: 16824.916360\n",
      "Epoch 57, loss: 16835.525580\n",
      "Epoch 58, loss: 16830.926742\n",
      "Epoch 59, loss: 16829.181988\n",
      "Epoch 60, loss: 16828.550137\n",
      "Epoch 61, loss: 16829.408835\n",
      "Epoch 62, loss: 16831.541669\n",
      "Epoch 63, loss: 16817.826184\n",
      "Epoch 64, loss: 16830.662437\n",
      "Epoch 65, loss: 16829.785590\n",
      "Epoch 66, loss: 16824.429030\n",
      "Epoch 67, loss: 16828.053498\n",
      "Epoch 68, loss: 16823.853599\n",
      "Epoch 69, loss: 16828.187851\n",
      "Epoch 70, loss: 16823.356274\n",
      "Epoch 71, loss: 16829.580367\n",
      "Epoch 72, loss: 16822.659629\n",
      "Epoch 73, loss: 16826.645856\n",
      "Epoch 74, loss: 16826.209505\n",
      "Epoch 75, loss: 16823.910303\n",
      "Epoch 76, loss: 16828.455498\n",
      "Epoch 77, loss: 16826.242973\n",
      "Epoch 78, loss: 16818.957749\n",
      "Epoch 79, loss: 16819.176308\n",
      "Epoch 80, loss: 16827.003506\n",
      "Epoch 81, loss: 16823.033156\n",
      "Epoch 82, loss: 16821.813508\n",
      "Epoch 83, loss: 16820.235984\n",
      "Epoch 84, loss: 16826.480629\n",
      "Epoch 85, loss: 16820.392905\n",
      "Epoch 86, loss: 16823.786117\n",
      "Epoch 87, loss: 16822.639672\n",
      "Epoch 88, loss: 16817.059035\n",
      "Epoch 89, loss: 16821.902382\n",
      "Epoch 90, loss: 16817.102645\n",
      "Epoch 91, loss: 16821.055635\n",
      "Epoch 92, loss: 16817.655250\n",
      "Epoch 93, loss: 16817.376728\n",
      "Epoch 94, loss: 16815.788664\n",
      "Epoch 95, loss: 16815.618565\n",
      "Epoch 96, loss: 16817.166598\n",
      "Epoch 97, loss: 16815.570498\n",
      "Epoch 98, loss: 16813.572668\n",
      "Epoch 99, loss: 16818.573747\n",
      "New best classifier with accuracy=0.247\n",
      "\n",
      "0.0001 1e-06\n",
      "Epoch 0, loss: 16816.599813\n",
      "Epoch 1, loss: 16817.526533\n",
      "Epoch 2, loss: 16815.451687\n",
      "Epoch 3, loss: 16814.896099\n",
      "Epoch 4, loss: 16814.659447\n",
      "Epoch 5, loss: 16816.567567\n",
      "Epoch 6, loss: 16812.471639\n",
      "Epoch 7, loss: 16816.019691\n",
      "Epoch 8, loss: 16818.332151\n",
      "Epoch 9, loss: 16816.336704\n",
      "Epoch 10, loss: 16817.928418\n",
      "Epoch 11, loss: 16813.629568\n",
      "Epoch 12, loss: 16814.287080\n",
      "Epoch 13, loss: 16813.435049\n",
      "Epoch 14, loss: 16817.700618\n",
      "Epoch 15, loss: 16811.251309\n",
      "Epoch 16, loss: 16811.657614\n",
      "Epoch 17, loss: 16810.834003\n",
      "Epoch 18, loss: 16814.473721\n",
      "Epoch 19, loss: 16810.477671\n",
      "Epoch 20, loss: 16814.517285\n",
      "Epoch 21, loss: 16809.693369\n",
      "Epoch 22, loss: 16810.115102\n",
      "Epoch 23, loss: 16811.165222\n",
      "Epoch 24, loss: 16811.185557\n",
      "Epoch 25, loss: 16809.656717\n",
      "Epoch 26, loss: 16810.221989\n",
      "Epoch 27, loss: 16808.399583\n",
      "Epoch 28, loss: 16810.096910\n",
      "Epoch 29, loss: 16808.904190\n",
      "Epoch 30, loss: 16805.339008\n",
      "Epoch 31, loss: 16805.769058\n",
      "Epoch 32, loss: 16811.415577\n",
      "Epoch 33, loss: 16807.996153\n",
      "Epoch 34, loss: 16812.138531\n",
      "Epoch 35, loss: 16812.496376\n",
      "Epoch 36, loss: 16801.758016\n",
      "Epoch 37, loss: 16806.418242\n",
      "Epoch 38, loss: 16803.852317\n",
      "Epoch 39, loss: 16807.474501\n",
      "Epoch 40, loss: 16810.382391\n",
      "Epoch 41, loss: 16804.645139\n",
      "Epoch 42, loss: 16806.654178\n",
      "Epoch 43, loss: 16808.586823\n",
      "Epoch 44, loss: 16803.081083\n",
      "Epoch 45, loss: 16803.880100\n",
      "Epoch 46, loss: 16804.571037\n",
      "Epoch 47, loss: 16804.304887\n",
      "Epoch 48, loss: 16806.745490\n",
      "Epoch 49, loss: 16804.826626\n",
      "Epoch 50, loss: 16803.532103\n",
      "Epoch 51, loss: 16803.539160\n",
      "Epoch 52, loss: 16799.538857\n",
      "Epoch 53, loss: 16805.173714\n",
      "Epoch 54, loss: 16802.396530\n",
      "Epoch 55, loss: 16800.267779\n",
      "Epoch 56, loss: 16803.220889\n",
      "Epoch 57, loss: 16802.478387\n",
      "Epoch 58, loss: 16800.585231\n",
      "Epoch 59, loss: 16797.116673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, loss: 16805.426962\n",
      "Epoch 61, loss: 16800.925552\n",
      "Epoch 62, loss: 16800.292354\n",
      "Epoch 63, loss: 16798.453747\n",
      "Epoch 64, loss: 16801.380825\n",
      "Epoch 65, loss: 16802.853194\n",
      "Epoch 66, loss: 16795.935946\n",
      "Epoch 67, loss: 16798.281756\n",
      "Epoch 68, loss: 16801.994862\n",
      "Epoch 69, loss: 16797.257621\n",
      "Epoch 70, loss: 16798.151156\n",
      "Epoch 71, loss: 16796.263920\n",
      "Epoch 72, loss: 16800.689711\n",
      "Epoch 73, loss: 16800.985354\n",
      "Epoch 74, loss: 16794.312422\n",
      "Epoch 75, loss: 16794.846915\n",
      "Epoch 76, loss: 16796.791067\n",
      "Epoch 77, loss: 16797.454908\n",
      "Epoch 78, loss: 16800.142619\n",
      "Epoch 79, loss: 16798.731066\n",
      "Epoch 80, loss: 16797.345541\n",
      "Epoch 81, loss: 16793.532344\n",
      "Epoch 82, loss: 16791.958876\n",
      "Epoch 83, loss: 16792.531045\n",
      "Epoch 84, loss: 16792.973293\n",
      "Epoch 85, loss: 16793.867898\n",
      "Epoch 86, loss: 16794.798382\n",
      "Epoch 87, loss: 16793.889790\n",
      "Epoch 88, loss: 16790.703713\n",
      "Epoch 89, loss: 16797.345245\n",
      "Epoch 90, loss: 16793.584685\n",
      "Epoch 91, loss: 16793.732165\n",
      "Epoch 92, loss: 16789.407156\n",
      "Epoch 93, loss: 16791.633024\n",
      "Epoch 94, loss: 16790.842106\n",
      "Epoch 95, loss: 16791.687157\n",
      "Epoch 96, loss: 16795.741952\n",
      "Epoch 97, loss: 16794.296772\n",
      "Epoch 98, loss: 16793.192876\n",
      "Epoch 99, loss: 16788.662863\n",
      "New best classifier with accuracy=0.253\n",
      "\n",
      "1e-05 0.0001\n",
      "Epoch 0, loss: 16767.967908\n",
      "Epoch 1, loss: 16765.707753\n",
      "Epoch 2, loss: 16765.415933\n",
      "Epoch 3, loss: 16764.833620\n",
      "Epoch 4, loss: 16765.150270\n",
      "Epoch 5, loss: 16764.713936\n",
      "Epoch 6, loss: 16765.179634\n",
      "Epoch 7, loss: 16764.806893\n",
      "Epoch 8, loss: 16765.265736\n",
      "Epoch 9, loss: 16764.759088\n",
      "Epoch 10, loss: 16764.902851\n",
      "Epoch 11, loss: 16764.728456\n",
      "Epoch 12, loss: 16764.777870\n",
      "Epoch 13, loss: 16764.574701\n",
      "Epoch 14, loss: 16764.596786\n",
      "Epoch 15, loss: 16764.602445\n",
      "Epoch 16, loss: 16764.487040\n",
      "Epoch 17, loss: 16764.740796\n",
      "Epoch 18, loss: 16764.411713\n",
      "Epoch 19, loss: 16764.568166\n",
      "Epoch 20, loss: 16764.731512\n",
      "Epoch 21, loss: 16764.631958\n",
      "Epoch 22, loss: 16764.492485\n",
      "Epoch 23, loss: 16764.621471\n",
      "Epoch 24, loss: 16764.848705\n",
      "Epoch 25, loss: 16764.140877\n",
      "Epoch 26, loss: 16764.979893\n",
      "Epoch 27, loss: 16764.425132\n",
      "Epoch 28, loss: 16764.454489\n",
      "Epoch 29, loss: 16764.216845\n",
      "Epoch 30, loss: 16764.404110\n",
      "Epoch 31, loss: 16764.225843\n",
      "Epoch 32, loss: 16764.144750\n",
      "Epoch 33, loss: 16764.007341\n",
      "Epoch 34, loss: 16763.973453\n",
      "Epoch 35, loss: 16764.275991\n",
      "Epoch 36, loss: 16764.592801\n",
      "Epoch 37, loss: 16764.314536\n",
      "Epoch 38, loss: 16764.141165\n",
      "Epoch 39, loss: 16764.038345\n",
      "Epoch 40, loss: 16764.435075\n",
      "Epoch 41, loss: 16764.293015\n",
      "Epoch 42, loss: 16764.551184\n",
      "Epoch 43, loss: 16764.401472\n",
      "Epoch 44, loss: 16763.983552\n",
      "Epoch 45, loss: 16763.903508\n",
      "Epoch 46, loss: 16763.803918\n",
      "Epoch 47, loss: 16763.729206\n",
      "Epoch 48, loss: 16763.727136\n",
      "Epoch 49, loss: 16763.569725\n",
      "Epoch 50, loss: 16763.947554\n",
      "Epoch 51, loss: 16763.577158\n",
      "Epoch 52, loss: 16763.999372\n",
      "Epoch 53, loss: 16763.791954\n",
      "Epoch 54, loss: 16763.972848\n",
      "Epoch 55, loss: 16763.597708\n",
      "Epoch 56, loss: 16763.758397\n",
      "Epoch 57, loss: 16763.765960\n",
      "Epoch 58, loss: 16764.052264\n",
      "Epoch 59, loss: 16763.600652\n",
      "Epoch 60, loss: 16763.747416\n",
      "Epoch 61, loss: 16763.761172\n",
      "Epoch 62, loss: 16764.167424\n",
      "Epoch 63, loss: 16763.832528\n",
      "Epoch 64, loss: 16763.416354\n",
      "Epoch 65, loss: 16763.615617\n",
      "Epoch 66, loss: 16763.401035\n",
      "Epoch 67, loss: 16763.449758\n",
      "Epoch 68, loss: 16763.233447\n",
      "Epoch 69, loss: 16763.564012\n",
      "Epoch 70, loss: 16763.747790\n",
      "Epoch 71, loss: 16763.448058\n",
      "Epoch 72, loss: 16763.361501\n",
      "Epoch 73, loss: 16763.552918\n",
      "Epoch 74, loss: 16763.031161\n",
      "Epoch 75, loss: 16763.150658\n",
      "Epoch 76, loss: 16763.128920\n",
      "Epoch 77, loss: 16763.575653\n",
      "Epoch 78, loss: 16763.279681\n",
      "Epoch 79, loss: 16762.862146\n",
      "Epoch 80, loss: 16763.575560\n",
      "Epoch 81, loss: 16763.118078\n",
      "Epoch 82, loss: 16762.816104\n",
      "Epoch 83, loss: 16763.221007\n",
      "Epoch 84, loss: 16762.934686\n",
      "Epoch 85, loss: 16763.380624\n",
      "Epoch 86, loss: 16763.246461\n",
      "Epoch 87, loss: 16762.550427\n",
      "Epoch 88, loss: 16763.020150\n",
      "Epoch 89, loss: 16762.867125\n",
      "Epoch 90, loss: 16763.206276\n",
      "Epoch 91, loss: 16762.764721\n",
      "Epoch 92, loss: 16762.827253\n",
      "Epoch 93, loss: 16762.909588\n",
      "Epoch 94, loss: 16762.651086\n",
      "Epoch 95, loss: 16763.191741\n",
      "Epoch 96, loss: 16762.614620\n",
      "Epoch 97, loss: 16762.625545\n",
      "Epoch 98, loss: 16762.778970\n",
      "Epoch 99, loss: 16762.632411\n",
      "\n",
      "1e-05 1e-05\n",
      "Epoch 0, loss: 16760.906627\n",
      "Epoch 1, loss: 16760.680109\n",
      "Epoch 2, loss: 16760.782196\n",
      "Epoch 3, loss: 16760.605827\n",
      "Epoch 4, loss: 16760.646414\n",
      "Epoch 5, loss: 16761.024055\n",
      "Epoch 6, loss: 16760.250017\n",
      "Epoch 7, loss: 16760.824178\n",
      "Epoch 8, loss: 16760.448879\n",
      "Epoch 9, loss: 16760.991776\n",
      "Epoch 10, loss: 16760.212073\n",
      "Epoch 11, loss: 16760.222548\n",
      "Epoch 12, loss: 16760.576640\n",
      "Epoch 13, loss: 16760.650407\n",
      "Epoch 14, loss: 16760.573765\n",
      "Epoch 15, loss: 16760.318306\n",
      "Epoch 16, loss: 16760.047538\n",
      "Epoch 17, loss: 16760.216563\n",
      "Epoch 18, loss: 16760.503599\n",
      "Epoch 19, loss: 16759.904553\n",
      "Epoch 20, loss: 16760.224223\n",
      "Epoch 21, loss: 16760.333791\n",
      "Epoch 22, loss: 16760.321231\n",
      "Epoch 23, loss: 16760.322522\n",
      "Epoch 24, loss: 16759.939929\n",
      "Epoch 25, loss: 16760.148831\n",
      "Epoch 26, loss: 16759.933204\n",
      "Epoch 27, loss: 16760.082888\n",
      "Epoch 28, loss: 16760.132061\n",
      "Epoch 29, loss: 16759.930192\n",
      "Epoch 30, loss: 16759.884915\n",
      "Epoch 31, loss: 16760.129251\n",
      "Epoch 32, loss: 16759.841261\n",
      "Epoch 33, loss: 16759.980225\n",
      "Epoch 34, loss: 16760.241918\n",
      "Epoch 35, loss: 16759.843511\n",
      "Epoch 36, loss: 16760.459009\n",
      "Epoch 37, loss: 16759.448804\n",
      "Epoch 38, loss: 16759.754959\n",
      "Epoch 39, loss: 16759.661223\n",
      "Epoch 40, loss: 16759.889373\n",
      "Epoch 41, loss: 16759.603518\n",
      "Epoch 42, loss: 16759.790938\n",
      "Epoch 43, loss: 16759.933534\n",
      "Epoch 44, loss: 16759.404695\n",
      "Epoch 45, loss: 16759.870463\n",
      "Epoch 46, loss: 16759.500425\n",
      "Epoch 47, loss: 16759.218074\n",
      "Epoch 48, loss: 16759.725422\n",
      "Epoch 49, loss: 16759.962638\n",
      "Epoch 50, loss: 16759.825480\n",
      "Epoch 51, loss: 16759.124031\n",
      "Epoch 52, loss: 16759.134342\n",
      "Epoch 53, loss: 16759.683727\n",
      "Epoch 54, loss: 16759.735776\n",
      "Epoch 55, loss: 16759.342587\n",
      "Epoch 56, loss: 16759.100615\n",
      "Epoch 57, loss: 16759.579113\n",
      "Epoch 58, loss: 16759.329964\n",
      "Epoch 59, loss: 16758.904297\n",
      "Epoch 60, loss: 16759.292555\n",
      "Epoch 61, loss: 16759.147686\n",
      "Epoch 62, loss: 16758.968267\n",
      "Epoch 63, loss: 16759.389128\n",
      "Epoch 64, loss: 16759.195409\n",
      "Epoch 65, loss: 16759.180839\n",
      "Epoch 66, loss: 16759.012678\n",
      "Epoch 67, loss: 16759.010080\n",
      "Epoch 68, loss: 16758.901741\n",
      "Epoch 69, loss: 16758.856893\n",
      "Epoch 70, loss: 16758.711223\n",
      "Epoch 71, loss: 16759.080908\n",
      "Epoch 72, loss: 16758.712698\n",
      "Epoch 73, loss: 16758.957820\n",
      "Epoch 74, loss: 16758.871637\n",
      "Epoch 75, loss: 16758.912651\n",
      "Epoch 76, loss: 16758.905835\n",
      "Epoch 77, loss: 16759.145869\n",
      "Epoch 78, loss: 16759.289403\n",
      "Epoch 79, loss: 16758.746788\n",
      "Epoch 80, loss: 16758.860270\n",
      "Epoch 81, loss: 16759.015078\n",
      "Epoch 82, loss: 16759.029490\n",
      "Epoch 83, loss: 16758.342353\n",
      "Epoch 84, loss: 16758.620536\n",
      "Epoch 85, loss: 16758.987849\n",
      "Epoch 86, loss: 16758.697822\n",
      "Epoch 87, loss: 16758.403409\n",
      "Epoch 88, loss: 16759.196991\n",
      "Epoch 89, loss: 16758.587499\n",
      "Epoch 90, loss: 16758.947021\n",
      "Epoch 91, loss: 16758.300932\n",
      "Epoch 92, loss: 16758.596694\n",
      "Epoch 93, loss: 16758.243070\n",
      "Epoch 94, loss: 16758.248323\n",
      "Epoch 95, loss: 16758.380653\n",
      "Epoch 96, loss: 16758.304812\n",
      "Epoch 97, loss: 16758.474456\n",
      "Epoch 98, loss: 16758.341320\n",
      "Epoch 99, loss: 16758.570036\n",
      "New best classifier with accuracy=0.254\n",
      "\n",
      "1e-05 1e-06\n",
      "Epoch 0, loss: 16758.358380\n",
      "Epoch 1, loss: 16758.018821\n",
      "Epoch 2, loss: 16758.005803\n",
      "Epoch 3, loss: 16758.393232\n",
      "Epoch 4, loss: 16757.981662\n",
      "Epoch 5, loss: 16757.969002\n",
      "Epoch 6, loss: 16758.182573\n",
      "Epoch 7, loss: 16757.836558\n",
      "Epoch 8, loss: 16758.058452\n",
      "Epoch 9, loss: 16758.229560\n",
      "Epoch 10, loss: 16757.867359\n",
      "Epoch 11, loss: 16758.107197\n",
      "Epoch 12, loss: 16758.089262\n",
      "Epoch 13, loss: 16758.022759\n",
      "Epoch 14, loss: 16758.012153\n",
      "Epoch 15, loss: 16757.488966\n",
      "Epoch 16, loss: 16757.563067\n",
      "Epoch 17, loss: 16757.654923\n",
      "Epoch 18, loss: 16757.881736\n",
      "Epoch 19, loss: 16757.561263\n",
      "Epoch 20, loss: 16757.726957\n",
      "Epoch 21, loss: 16757.435066\n",
      "Epoch 22, loss: 16757.433932\n",
      "Epoch 23, loss: 16757.599678\n",
      "Epoch 24, loss: 16757.874982\n",
      "Epoch 25, loss: 16757.841784\n",
      "Epoch 26, loss: 16757.818670\n",
      "Epoch 27, loss: 16757.499165\n",
      "Epoch 28, loss: 16757.561367\n",
      "Epoch 29, loss: 16757.493670\n",
      "Epoch 30, loss: 16757.427799\n",
      "Epoch 31, loss: 16757.803164\n",
      "Epoch 32, loss: 16757.291835\n",
      "Epoch 33, loss: 16757.900597\n",
      "Epoch 34, loss: 16756.999644\n",
      "Epoch 35, loss: 16757.011815\n",
      "Epoch 36, loss: 16757.661684\n",
      "Epoch 37, loss: 16757.036109\n",
      "Epoch 38, loss: 16757.410940\n",
      "Epoch 39, loss: 16756.953234\n",
      "Epoch 40, loss: 16757.621508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, loss: 16757.465174\n",
      "Epoch 42, loss: 16757.496809\n",
      "Epoch 43, loss: 16757.015550\n",
      "Epoch 44, loss: 16756.822115\n",
      "Epoch 45, loss: 16757.087900\n",
      "Epoch 46, loss: 16756.952546\n",
      "Epoch 47, loss: 16757.104473\n",
      "Epoch 48, loss: 16756.954657\n",
      "Epoch 49, loss: 16756.907112\n",
      "Epoch 50, loss: 16756.868770\n",
      "Epoch 51, loss: 16757.239839\n",
      "Epoch 52, loss: 16756.765988\n",
      "Epoch 53, loss: 16756.677606\n",
      "Epoch 54, loss: 16756.841204\n",
      "Epoch 55, loss: 16756.891479\n",
      "Epoch 56, loss: 16756.889475\n",
      "Epoch 57, loss: 16757.225715\n",
      "Epoch 58, loss: 16756.805564\n",
      "Epoch 59, loss: 16756.876191\n",
      "Epoch 60, loss: 16756.587263\n",
      "Epoch 61, loss: 16756.445147\n",
      "Epoch 62, loss: 16756.610496\n",
      "Epoch 63, loss: 16756.551447\n",
      "Epoch 64, loss: 16756.315257\n",
      "Epoch 65, loss: 16756.483201\n",
      "Epoch 66, loss: 16757.184918\n",
      "Epoch 67, loss: 16756.795333\n",
      "Epoch 68, loss: 16756.261324\n",
      "Epoch 69, loss: 16756.510598\n",
      "Epoch 70, loss: 16756.408188\n",
      "Epoch 71, loss: 16756.775359\n",
      "Epoch 72, loss: 16756.615711\n",
      "Epoch 73, loss: 16756.382258\n",
      "Epoch 74, loss: 16756.501830\n",
      "Epoch 75, loss: 16756.031319\n",
      "Epoch 76, loss: 16756.843690\n",
      "Epoch 77, loss: 16756.423101\n",
      "Epoch 78, loss: 16756.462099\n",
      "Epoch 79, loss: 16756.446659\n",
      "Epoch 80, loss: 16756.243081\n",
      "Epoch 81, loss: 16755.995023\n",
      "Epoch 82, loss: 16756.216109\n",
      "Epoch 83, loss: 16756.389039\n",
      "Epoch 84, loss: 16756.573188\n",
      "Epoch 85, loss: 16756.280680\n",
      "Epoch 86, loss: 16756.267377\n",
      "Epoch 87, loss: 16755.808718\n",
      "Epoch 88, loss: 16756.128030\n",
      "Epoch 89, loss: 16755.903020\n",
      "Epoch 90, loss: 16756.130359\n",
      "Epoch 91, loss: 16756.257907\n",
      "Epoch 92, loss: 16755.720148\n",
      "Epoch 93, loss: 16756.142291\n",
      "Epoch 94, loss: 16755.997257\n",
      "Epoch 95, loss: 16756.246217\n",
      "Epoch 96, loss: 16755.796899\n",
      "Epoch 97, loss: 16755.855842\n",
      "Epoch 98, loss: 16755.793878\n",
      "Epoch 99, loss: 16755.952961\n",
      "\n",
      "best validation accuracy achieved: 0.254000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = -np.inf\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for learning_rate in learning_rates:\n",
    "    for reg_strength in reg_strengths:\n",
    "        print(learning_rate, reg_strength)\n",
    "        classifier.fit(train_X, train_y, epochs=100, learning_rate=learning_rate, batch_size=300, reg=reg_strength)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        \n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "            print('New best classifier with accuracy={}'.format(best_val_accuracy))\n",
    "        print()\n",
    "        \n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.193000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
